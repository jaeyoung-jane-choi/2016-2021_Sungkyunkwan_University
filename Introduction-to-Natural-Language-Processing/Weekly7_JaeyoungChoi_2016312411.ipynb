{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Weekly7_JaeyoungChoi_2016312411.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdA9S1UAcr_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os.path\n",
        "import pandas as pd \n",
        "import nltk\n",
        "# nltk.download('all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zzWJq-fgMBx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "outputId": "413ef276-5526-4b43-cd37-55c168768c47"
      },
      "source": [
        "#loading the data \n",
        "df = pd.read_csv('amazon_reviews.csv')\n",
        "df[:10]"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1303862400</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1346976000</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1219017600</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1307923200</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1350777600</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>ADT0SRK1MGOEU</td>\n",
              "      <td>Twoapennything</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1342051200</td>\n",
              "      <td>Nice Taffy</td>\n",
              "      <td>I got a wild hair for taffy and ordered this f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1SP2KVKFXXRU1</td>\n",
              "      <td>David C. Sullivan</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1340150400</td>\n",
              "      <td>Great!  Just as good as the expensive brands!</td>\n",
              "      <td>This saltwater taffy had great flavors and was...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A3JRGQVEQN31IQ</td>\n",
              "      <td>Pamela G. Williams</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1336003200</td>\n",
              "      <td>Wonderful, tasty taffy</td>\n",
              "      <td>This taffy is so good.  It is very soft and ch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>B000E7L2R4</td>\n",
              "      <td>A1MZYO9TZK0BBI</td>\n",
              "      <td>R. James</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1322006400</td>\n",
              "      <td>Yay Barley</td>\n",
              "      <td>Right now I'm mostly just sprouting this so my...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>B00171APVA</td>\n",
              "      <td>A21BT40VZCCYT4</td>\n",
              "      <td>Carol A. Reed</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1351209600</td>\n",
              "      <td>Healthy Dog Food</td>\n",
              "      <td>This is a very healthy dog food. Good for thei...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  ...                                               Text\n",
              "0   1  ...  I have bought several of the Vitality canned d...\n",
              "1   2  ...  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2   3  ...  This is a confection that has been around a fe...\n",
              "3   4  ...  If you are looking for the secret ingredient i...\n",
              "4   5  ...  Great taffy at a great price.  There was a wid...\n",
              "5   6  ...  I got a wild hair for taffy and ordered this f...\n",
              "6   7  ...  This saltwater taffy had great flavors and was...\n",
              "7   8  ...  This taffy is so good.  It is very soft and ch...\n",
              "8   9  ...  Right now I'm mostly just sprouting this so my...\n",
              "9  10  ...  This is a very healthy dog food. Good for thei...\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF8GLl1pjdvQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "0ef992e0-b247-44c1-d032-9daebc98858d"
      },
      "source": [
        "#Lets reduce the dataset: only use text \n",
        "df_text = df['Text']\n",
        "df_text.shape \n",
        "# type(df_text)#pd series \n",
        "# type(df_text [0]) #str\n",
        "\n",
        "\n",
        "#too large dataset reduce dataset to 20000\n",
        "df_text = df_text[:20000]\n",
        "df_text.shape"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdA5hsU6kGvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preprocess the data \n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re \n",
        "\n",
        "def preprocess_data(df):\n",
        "  \"\"\"\n",
        "  input:: df pd series \n",
        "  output:: nested list \n",
        "  \"\"\"\n",
        "  #1 tockenize the string words \n",
        "  #2 loop through stop words -delete stop words\n",
        "  #3 append to a new list\n",
        "\n",
        "  tokenlist =[]\n",
        "  stop = set(stopwords.words('english'))\n",
        "  for row,line in enumerate(df): \n",
        "    line = line.lower()\n",
        "    #delete non-words \n",
        "    line = re.sub(pattern = '[\\W]',repl=' ',string = line )\n",
        "    # print(line)\n",
        "    #delete one letter length such as c, w ... \n",
        "    line  = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", string = line )\n",
        "    # print(line)\n",
        "    line = re.sub(r\"\\bamazon\\b\", \"\", string = line )\n",
        "    # print(line)\n",
        "    line = re.sub(r\"\\bproduct\\b\", \"\", string = line )\n",
        "\n",
        "    #tokenize the string words\n",
        "    tokens = nltk.word_tokenize(line) \n",
        "    #loop through stop words\n",
        "    wordlist=[] \n",
        "    for word in tokens:\n",
        "      if word not in stop:\n",
        "        #not allows duplicates \n",
        "        if word not in wordlist:\n",
        "          wordlist.append(word)\n",
        "      \n",
        "    tokenlist.append(wordlist)\n",
        "\n",
        "  \n",
        "      \n",
        "  return tokenlist\n",
        "  \n",
        "# df_simple = df_text[:10]\n",
        "\n",
        "# t = preprocess_data(df_simple)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lsNgpd1vA2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#vectorizing the dataset \n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#since we have tokenized the data above \n",
        "def tok(text):\n",
        "  return text\n",
        "\n",
        "def vectorize(df):\n",
        "  \"\"\"\n",
        "  input:: nested list of words (preprocessed data)\n",
        "  output:: dic of 1. vectorized words type of numpy array, 2. features of the vectorizer \n",
        "  \"\"\"\n",
        "  vectorizer = TfidfVectorizer(lowercase=False, tokenizer=tok,max_features= 1000) #cut to top 1000 words \n",
        "  vector_data = vectorizer.fit_transform(df)\n",
        "  array_vector = vector_data.toarray()\n",
        "  #array_vector.shape  \n",
        "  #(num_doc , num words)\n",
        "  array_vector = array_vector #transpose as the SVD ppt \n",
        "\n",
        "  features = vectorizer.get_feature_names()\n",
        "  vectorized ={}\n",
        "  vectorized['vector'] = array_vector\n",
        "\n",
        "  vectorized['feature'] = features\n",
        "  \n",
        "  return vectorized \n",
        "\n",
        "# vec = vectorize(t)['vector']\n",
        "# feat = vectorize(t)['feature']\n",
        "\n",
        "# v = vectorize(t)\n",
        "# vec.shape #shape(10:documents,171:term(words))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUBZC-rp0Gc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Topic Modeling with SVD \n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "\n",
        "def modeling(np,num):\n",
        "  \"\"\"\n",
        "  input:: numpy array of (doc,word) , num(int): n_componenets dictionary \n",
        "  output:: dictionary 1) np array (topic,doc) 2) features \n",
        "\n",
        "  \"\"\"\n",
        "  #don't know how many topics are inside the dataset so randomly choosed 20\n",
        "  vect = np['vector'] #assign matrix \n",
        "  \n",
        "  tsvd_model = TruncatedSVD(n_components=num, algorithm='randomized', n_iter=100, random_state=122)\n",
        "  tsvd_model.fit(vect) #fit the vectorized tfidf array\n",
        "  vt = tsvd_model.components_\n",
        "  # print(vt.shape) \n",
        "  # print(vt)\n",
        "\n",
        "  modeled ={}\n",
        "  modeled['tsvd'] = vt\n",
        "  modeled['feature'] = np['feature']\n",
        "\n",
        "  return modeled\n",
        "\n",
        "# a = modeling(v,5)['tsvd'] \n",
        "# a.shape #shape is (5,171) since we set the topic num to 5 , words = 171\n",
        "# new =modeling(v,5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhzFCeWu0xQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#infer the topic by the top rated words \n",
        "len(feat) #list of words length of 171 \n",
        "\n",
        "def infer(svd_dic):\n",
        "  \"\"\"\n",
        "  input:: svd_dic (1) tsvd : vectorized matrix , (2) feature : features list \n",
        "  output::  NONE print out form \n",
        "  \"\"\"\n",
        "  tsvd = svd_dic['tsvd']\n",
        "  feature =svd_dic['feature']\n",
        "\n",
        "  final_dic ={}\n",
        "  for top_num, row in enumerate(tsvd): #for each row of tsvd \n",
        "    \n",
        "    sort = sorted(row, reverse=True) #sort row of tsvd \n",
        "    \n",
        "    sort_index = np.argsort(row) #get index number of the sorted row \n",
        "    top_ten_value = sort[:10] #cut the top 10 of each value \n",
        "    top_ten_index = sort_index[:10]\n",
        "    \n",
        "    \n",
        "    #get the word for the index from the feature list \n",
        "    wordlist=[]\n",
        "    for index in top_ten_index : \n",
        "      wordlist.append(feature[index])\n",
        "\n",
        "    \n",
        "    #top_ten_value, wordlist for row \n",
        "\n",
        "    for i in range(10):\n",
        "\n",
        "      print('Topic: {}, Word: {} , Value: {}'.format(top_num+1, wordlist[i],top_ten_value[i].round(6)))\n",
        "\n",
        "    \n",
        "\n",
        "# infer(new)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE18k8vN83E4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        },
        "outputId": "e529c4ee-c9de-477e-9efa-496fbc4f2090"
      },
      "source": [
        "#USE DATASET FOR TOPIC MODELING \n",
        "\n",
        "df_clean = preprocess_data(df_text)\n",
        "clean_vect_dic = vectorize(df_clean)\n",
        "\n",
        "model = modeling(clean_vect_dic,5) #5 topics \n",
        "infer(model)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 1, Word: nutritional , Value: 0.174041\n",
            "Topic: 1, Word: main , Value: 0.168937\n",
            "Topic: 1, Word: seconds , Value: 0.160701\n",
            "Topic: 1, Word: fridge , Value: 0.148957\n",
            "Topic: 1, Word: outside , Value: 0.143802\n",
            "Topic: 1, Word: form , Value: 0.140243\n",
            "Topic: 1, Word: bits , Value: 0.133185\n",
            "Topic: 1, Word: total , Value: 0.132884\n",
            "Topic: 1, Word: level , Value: 0.123829\n",
            "Topic: 1, Word: air , Value: 0.121314\n",
            "Topic: 2, Word: dog , Value: 0.485529\n",
            "Topic: 2, Word: food , Value: 0.274918\n",
            "Topic: 2, Word: treats , Value: 0.168444\n",
            "Topic: 2, Word: eat , Value: 0.168167\n",
            "Topic: 2, Word: dogs , Value: 0.160797\n",
            "Topic: 2, Word: loves , Value: 0.141553\n",
            "Topic: 2, Word: healthy , Value: 0.135855\n",
            "Topic: 2, Word: old , Value: 0.131267\n",
            "Topic: 2, Word: treat , Value: 0.130914\n",
            "Topic: 2, Word: free , Value: 0.128599\n",
            "Topic: 3, Word: coffee , Value: 0.221891\n",
            "Topic: 3, Word: dog , Value: 0.185084\n",
            "Topic: 3, Word: loves , Value: 0.181509\n",
            "Topic: 3, Word: price , Value: 0.163735\n",
            "Topic: 3, Word: treats , Value: 0.159846\n",
            "Topic: 3, Word: dogs , Value: 0.152403\n",
            "Topic: 3, Word: food , Value: 0.151937\n",
            "Topic: 3, Word: time , Value: 0.13407\n",
            "Topic: 3, Word: best , Value: 0.121205\n",
            "Topic: 3, Word: order , Value: 0.12061\n",
            "Topic: 4, Word: dog , Value: 0.475275\n",
            "Topic: 4, Word: one , Value: 0.292165\n",
            "Topic: 4, Word: br , Value: 0.173739\n",
            "Topic: 4, Word: drink , Value: 0.159338\n",
            "Topic: 4, Word: treats , Value: 0.121081\n",
            "Topic: 4, Word: cup , Value: 0.117372\n",
            "Topic: 4, Word: would , Value: 0.116539\n",
            "Topic: 4, Word: 3 , Value: 0.113205\n",
            "Topic: 4, Word: coffee , Value: 0.108767\n",
            "Topic: 4, Word: juice , Value: 0.108041\n",
            "Topic: 5, Word: price , Value: 0.281291\n",
            "Topic: 5, Word: store , Value: 0.256828\n",
            "Topic: 5, Word: order , Value: 0.243075\n",
            "Topic: 5, Word: grocery , Value: 0.226127\n",
            "Topic: 5, Word: local , Value: 0.212598\n",
            "Topic: 5, Word: find , Value: 0.166171\n",
            "Topic: 5, Word: buy , Value: 0.135337\n",
            "Topic: 5, Word: shipping , Value: 0.11502\n",
            "Topic: 5, Word: ordered , Value: 0.099646\n",
            "Topic: 5, Word: bought , Value: 0.094015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLDIpwuWN672",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1. Topic1: nutritional product?\n",
        "2. Topic2: a treat that a dog likes\n",
        "3. Topic3: coffee related, good reviews \n",
        "4. Topic4: drink? \n",
        "5. Topic5: grocery related?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2W-vZmAMkdQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "55933d7d-ed1b-4cff-cb76-292ad8452470"
      },
      "source": [
        "model2 = modeling(clean_vect_dic,10) #try 10 topics \n",
        "infer(model2)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 1, Word: nutritional , Value: 0.174041\n",
            "Topic: 1, Word: main , Value: 0.168937\n",
            "Topic: 1, Word: seconds , Value: 0.160701\n",
            "Topic: 1, Word: fridge , Value: 0.148957\n",
            "Topic: 1, Word: outside , Value: 0.143802\n",
            "Topic: 1, Word: form , Value: 0.140243\n",
            "Topic: 1, Word: bits , Value: 0.133185\n",
            "Topic: 1, Word: total , Value: 0.132884\n",
            "Topic: 1, Word: level , Value: 0.123829\n",
            "Topic: 1, Word: air , Value: 0.121314\n",
            "Topic: 2, Word: dog , Value: 0.485529\n",
            "Topic: 2, Word: food , Value: 0.274918\n",
            "Topic: 2, Word: treats , Value: 0.168444\n",
            "Topic: 2, Word: eat , Value: 0.168167\n",
            "Topic: 2, Word: dogs , Value: 0.160797\n",
            "Topic: 2, Word: loves , Value: 0.141553\n",
            "Topic: 2, Word: healthy , Value: 0.135855\n",
            "Topic: 2, Word: old , Value: 0.131267\n",
            "Topic: 2, Word: treat , Value: 0.130914\n",
            "Topic: 2, Word: free , Value: 0.128599\n",
            "Topic: 3, Word: coffee , Value: 0.221891\n",
            "Topic: 3, Word: dog , Value: 0.185084\n",
            "Topic: 3, Word: loves , Value: 0.181509\n",
            "Topic: 3, Word: price , Value: 0.163735\n",
            "Topic: 3, Word: treats , Value: 0.159846\n",
            "Topic: 3, Word: dogs , Value: 0.152403\n",
            "Topic: 3, Word: food , Value: 0.151937\n",
            "Topic: 3, Word: time , Value: 0.13407\n",
            "Topic: 3, Word: best , Value: 0.121205\n",
            "Topic: 3, Word: order , Value: 0.12061\n",
            "Topic: 4, Word: dog , Value: 0.475275\n",
            "Topic: 4, Word: one , Value: 0.292165\n",
            "Topic: 4, Word: br , Value: 0.173739\n",
            "Topic: 4, Word: drink , Value: 0.159338\n",
            "Topic: 4, Word: treats , Value: 0.121081\n",
            "Topic: 4, Word: cup , Value: 0.117372\n",
            "Topic: 4, Word: would , Value: 0.116539\n",
            "Topic: 4, Word: 3 , Value: 0.113205\n",
            "Topic: 4, Word: coffee , Value: 0.108767\n",
            "Topic: 4, Word: juice , Value: 0.108041\n",
            "Topic: 5, Word: price , Value: 0.281291\n",
            "Topic: 5, Word: store , Value: 0.256828\n",
            "Topic: 5, Word: order , Value: 0.243075\n",
            "Topic: 5, Word: grocery , Value: 0.226127\n",
            "Topic: 5, Word: local , Value: 0.212598\n",
            "Topic: 5, Word: find , Value: 0.166171\n",
            "Topic: 5, Word: buy , Value: 0.135337\n",
            "Topic: 5, Word: shipping , Value: 0.11502\n",
            "Topic: 5, Word: ordered , Value: 0.099646\n",
            "Topic: 5, Word: bought , Value: 0.094015\n",
            "Topic: 6, Word: great , Value: 0.438874\n",
            "Topic: 6, Word: price , Value: 0.277924\n",
            "Topic: 6, Word: good , Value: 0.264348\n",
            "Topic: 6, Word: treats , Value: 0.150804\n",
            "Topic: 6, Word: dog , Value: 0.149012\n",
            "Topic: 6, Word: tea , Value: 0.132133\n",
            "Topic: 6, Word: love , Value: 0.125686\n",
            "Topic: 6, Word: dogs , Value: 0.107034\n",
            "Topic: 6, Word: coffee , Value: 0.105235\n",
            "Topic: 6, Word: drink , Value: 0.103773\n",
            "Topic: 7, Word: good , Value: 0.241372\n",
            "Topic: 7, Word: buy , Value: 0.171967\n",
            "Topic: 7, Word: price , Value: 0.16023\n",
            "Topic: 7, Word: would , Value: 0.150823\n",
            "Topic: 7, Word: best , Value: 0.145037\n",
            "Topic: 7, Word: better , Value: 0.144871\n",
            "Topic: 7, Word: really , Value: 0.143103\n",
            "Topic: 7, Word: store , Value: 0.141286\n",
            "Topic: 7, Word: like , Value: 0.14111\n",
            "Topic: 7, Word: flavor , Value: 0.125289\n",
            "Topic: 8, Word: good , Value: 0.263163\n",
            "Topic: 8, Word: use , Value: 0.20904\n",
            "Topic: 8, Word: like , Value: 0.173415\n",
            "Topic: 8, Word: make , Value: 0.161217\n",
            "Topic: 8, Word: taste , Value: 0.160578\n",
            "Topic: 8, Word: really , Value: 0.1591\n",
            "Topic: 8, Word: easy , Value: 0.156697\n",
            "Topic: 8, Word: cup , Value: 0.146942\n",
            "Topic: 8, Word: would , Value: 0.139482\n",
            "Topic: 8, Word: little , Value: 0.138052\n",
            "Topic: 9, Word: free , Value: 0.404243\n",
            "Topic: 9, Word: price , Value: 0.34181\n",
            "Topic: 9, Word: good , Value: 0.171074\n",
            "Topic: 9, Word: gluten , Value: 0.125048\n",
            "Topic: 9, Word: best , Value: 0.105137\n",
            "Topic: 9, Word: loves , Value: 0.089479\n",
            "Topic: 9, Word: coffee , Value: 0.088828\n",
            "Topic: 9, Word: com , Value: 0.087772\n",
            "Topic: 9, Word: www , Value: 0.08247\n",
            "Topic: 9, Word: http , Value: 0.082459\n",
            "Topic: 10, Word: chips , Value: 0.260684\n",
            "Topic: 10, Word: love , Value: 0.205132\n",
            "Topic: 10, Word: pack , Value: 0.194362\n",
            "Topic: 10, Word: snack , Value: 0.181297\n",
            "Topic: 10, Word: bag , Value: 0.150177\n",
            "Topic: 10, Word: order , Value: 0.12607\n",
            "Topic: 10, Word: flavor , Value: 0.125168\n",
            "Topic: 10, Word: com , Value: 0.116462\n",
            "Topic: 10, Word: eat , Value: 0.106976\n",
            "Topic: 10, Word: salt , Value: 0.101888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3rTOKDrMz1x",
        "colab_type": "text"
      },
      "source": [
        "Topic10: looks like chips \n",
        " "
      ]
    }
  ]
}