{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Weekly12-NLP-RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Whp-iVoZNLH0",
        "colab_type": "text"
      },
      "source": [
        "# **BASIC RNN CODE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDXYXAZ63tjY",
        "colab_type": "code",
        "outputId": "baa3fc9c-434e-4591-ee89-11b001a0f2be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN\n",
        "\n",
        "#BUILDING A MODEL USING SIMPLE RNN \n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(3, input_shape=(2,10))) #hidden_size = 3 layer \n",
        "\n",
        "# same expression :: model.add(SimpleRNN(3, input_length=2, input_dim=10)) :splitting the input shape to length, dim\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_1 (SimpleRNN)     (None, 3)                 42        \n",
            "=================================================================\n",
            "Total params: 42\n",
            "Trainable params: 42\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybabtaHCMRQF",
        "colab_type": "text"
      },
      "source": [
        "〽️ The total Parameters are 42.\n",
        "\n",
        "〽️ The output shape is (none, hiddenlayersize) : can be adjusted by specifing the batch_size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiDP5U8cMwFb",
        "colab_type": "code",
        "outputId": "d4058058-f54f-4e1f-a2ce-536ace353828",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(SimpleRNN(3, batch_input_shape=(8,2,10)))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_2 (SimpleRNN)     (8, 3)                    42        \n",
            "=================================================================\n",
            "Total params: 42\n",
            "Trainable params: 42\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKBTIsaiMw5m",
        "colab_type": "text"
      },
      "source": [
        "〽️ The total Parameters are 42.\n",
        "\n",
        "〽️ The batch input size is specified as 8,2,10 which gives the output shape as (8, hiddenlayersize)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlhI0J7NM9iZ",
        "colab_type": "code",
        "outputId": "6b20793a-6380-4765-bef0-c8849c752758",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(SimpleRNN(3, batch_input_shape=(8,2,10), return_sequences=True))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_3 (SimpleRNN)     (8, 2, 3)                 42        \n",
            "=================================================================\n",
            "Total params: 42\n",
            "Trainable params: 42\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFjTandSM-No",
        "colab_type": "text"
      },
      "source": [
        "〽️ Specifing return_sequences to True makes the output shape to have a 3 dimensional shape \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpyw1nURNZ-n",
        "colab_type": "text"
      },
      "source": [
        "# **BASIC LSTM**\n",
        "\n",
        "\n",
        "〽️ Basic RNN problem (problem of Long-Term Dependencies) :As the time-step becomes deeper, looses earlier input data \n",
        "\n",
        "〽️ To improve data memory -> LSTM (Long Sort-Term Memory)  \n",
        "\n",
        "〽️ Has input, output, delete steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0n_EZ7UOsRE",
        "colab_type": "text"
      },
      "source": [
        "# **RNN ANALYSIS**\n",
        "\n",
        "〽️ Many-to-one RNN \n",
        "\n",
        "〽️ Predicting next word after sequence \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJD7i3EwNGEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PREPROCESSING DATA \n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "text=\"\"\"경마장에 있는 말이 뛰고 있다\\n\n",
        "그의 말이 법이다\\n\n",
        "가는 말이 고와야 오는 말이 곱다\\n\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF357wHgPNDw",
        "colab_type": "code",
        "outputId": "96eca14c-6756-4769-d239-adc42b6f4aad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "\n",
        "\n",
        "t = Tokenizer() #tockenize the textdata \n",
        "t.fit_on_texts([text])\n",
        "\n",
        "\n",
        "vocab_size = len(t.word_index) + 1 \n",
        "\n",
        "print('단어 집합의 크기 : %d' % vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X08i6kP-Pcpo",
        "colab_type": "code",
        "outputId": "a3279567-c6c5-42d9-de35-38734a041759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        " #there are 11 words  in the tockenized dictionary \n",
        "\n",
        "t.word_index"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'가는': 8,\n",
              " '경마장에': 2,\n",
              " '고와야': 9,\n",
              " '곱다': 11,\n",
              " '그의': 6,\n",
              " '뛰고': 4,\n",
              " '말이': 1,\n",
              " '법이다': 7,\n",
              " '오는': 10,\n",
              " '있는': 3,\n",
              " '있다': 5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeVRs-21PzGp",
        "colab_type": "code",
        "outputId": "101633a0-a272-47fd-c289-26f659d95924",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "#Making the train-data-set \n",
        "\n",
        "sequences = list()\n",
        "\n",
        "for line in text.split('\\n'): #split according to \\n  \n",
        "    encoded = t.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "print('학습에 사용할 샘플의 개수: %d' % len(sequences))\n",
        "\n",
        "print(sequences) #the sequence are printed out as the index "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습에 사용할 샘플의 개수: 11\n",
            "[[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCeQZomqQO99",
        "colab_type": "code",
        "outputId": "189be776-e258-41b7-984b-97be1bcffaab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "max_len=max(len(l) for l in sequences) #get the longest sample\n",
        "print('샘플의 최대 길이 : {}'.format(max_len))\n",
        "\n",
        "#according to the longest sample , padd the samples\n",
        "\n",
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "\n",
        "print(sequences) #all are length of 6 with zeros inside "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "샘플의 최대 길이 : 6\n",
            "[[ 0  0  0  0  2  3]\n",
            " [ 0  0  0  2  3  1]\n",
            " [ 0  0  2  3  1  4]\n",
            " [ 0  2  3  1  4  5]\n",
            " [ 0  0  0  0  6  1]\n",
            " [ 0  0  0  6  1  7]\n",
            " [ 0  0  0  0  8  1]\n",
            " [ 0  0  0  8  1  9]\n",
            " [ 0  0  8  1  9 10]\n",
            " [ 0  8  1  9 10  1]\n",
            " [ 8  1  9 10  1 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zshNUf2gQc0C",
        "colab_type": "code",
        "outputId": "5b1db942-14dc-423a-ac44-24e69dd1dc95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "#split the x,y \n",
        "sequences = np.array(sequences)\n",
        "X = sequences[:,:-1]\n",
        "y = sequences[:,-1]\n",
        "\n",
        "print(X)\n",
        "\n",
        "print(y) #next word after the x's "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  0  2]\n",
            " [ 0  0  0  2  3]\n",
            " [ 0  0  2  3  1]\n",
            " [ 0  2  3  1  4]\n",
            " [ 0  0  0  0  6]\n",
            " [ 0  0  0  6  1]\n",
            " [ 0  0  0  0  8]\n",
            " [ 0  0  0  8  1]\n",
            " [ 0  0  8  1  9]\n",
            " [ 0  8  1  9 10]\n",
            " [ 8  1  9 10  1]]\n",
            "[ 3  1  4  5  1  7  1  9 10  1 11]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNisNEfnQlXJ",
        "colab_type": "code",
        "outputId": "2119163b-01c0-4131-af24-2a9cebaaa83a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "#one-hot encode the y's \n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "print(y)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBNujCJ7Qpg_",
        "colab_type": "code",
        "outputId": "6437f479-1bc9-4f1a-8007-148272d5fcb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Modeling with RNN\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "#vocab_size = len(t.word_index) + 1 :because keras-one-hot-encoding index starts at 0 \n",
        "\n",
        "\n",
        "model.add(Embedding(vocab_size, 10, input_length=max_len-1)) #embedd into 10 dimenstion, with input length of 5(since we splitted x,y)\n",
        "\n",
        "model.add(SimpleRNN(32)) #hidden units 32 \n",
        "\n",
        "model.add(Dense(vocab_size, activation='softmax')) #output layer \n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=200, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 0s - loss: 2.4609 - accuracy: 0.1818\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 2.4491 - accuracy: 0.2727\n",
            "Epoch 3/200\n",
            "1/1 - 0s - loss: 2.4374 - accuracy: 0.2727\n",
            "Epoch 4/200\n",
            "1/1 - 0s - loss: 2.4255 - accuracy: 0.2727\n",
            "Epoch 5/200\n",
            "1/1 - 0s - loss: 2.4133 - accuracy: 0.3636\n",
            "Epoch 6/200\n",
            "1/1 - 0s - loss: 2.4009 - accuracy: 0.4545\n",
            "Epoch 7/200\n",
            "1/1 - 0s - loss: 2.3879 - accuracy: 0.4545\n",
            "Epoch 8/200\n",
            "1/1 - 0s - loss: 2.3744 - accuracy: 0.4545\n",
            "Epoch 9/200\n",
            "1/1 - 0s - loss: 2.3603 - accuracy: 0.4545\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 2.3455 - accuracy: 0.4545\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 2.3300 - accuracy: 0.4545\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 2.3136 - accuracy: 0.4545\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 2.2963 - accuracy: 0.4545\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 2.2782 - accuracy: 0.4545\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 2.2590 - accuracy: 0.4545\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 2.2388 - accuracy: 0.4545\n",
            "Epoch 17/200\n",
            "1/1 - 0s - loss: 2.2176 - accuracy: 0.4545\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 2.1954 - accuracy: 0.4545\n",
            "Epoch 19/200\n",
            "1/1 - 0s - loss: 2.1721 - accuracy: 0.4545\n",
            "Epoch 20/200\n",
            "1/1 - 0s - loss: 2.1479 - accuracy: 0.4545\n",
            "Epoch 21/200\n",
            "1/1 - 0s - loss: 2.1228 - accuracy: 0.4545\n",
            "Epoch 22/200\n",
            "1/1 - 0s - loss: 2.0969 - accuracy: 0.4545\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 2.0703 - accuracy: 0.3636\n",
            "Epoch 24/200\n",
            "1/1 - 0s - loss: 2.0432 - accuracy: 0.3636\n",
            "Epoch 25/200\n",
            "1/1 - 0s - loss: 2.0159 - accuracy: 0.3636\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 1.9885 - accuracy: 0.3636\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 1.9613 - accuracy: 0.3636\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 1.9346 - accuracy: 0.3636\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 1.9088 - accuracy: 0.3636\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 1.8839 - accuracy: 0.3636\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 1.8604 - accuracy: 0.3636\n",
            "Epoch 32/200\n",
            "1/1 - 0s - loss: 1.8382 - accuracy: 0.3636\n",
            "Epoch 33/200\n",
            "1/1 - 0s - loss: 1.8176 - accuracy: 0.3636\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 1.7984 - accuracy: 0.3636\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 1.7806 - accuracy: 0.3636\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 1.7639 - accuracy: 0.3636\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 1.7481 - accuracy: 0.3636\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 1.7327 - accuracy: 0.3636\n",
            "Epoch 39/200\n",
            "1/1 - 0s - loss: 1.7174 - accuracy: 0.3636\n",
            "Epoch 40/200\n",
            "1/1 - 0s - loss: 1.7020 - accuracy: 0.3636\n",
            "Epoch 41/200\n",
            "1/1 - 0s - loss: 1.6862 - accuracy: 0.3636\n",
            "Epoch 42/200\n",
            "1/1 - 0s - loss: 1.6698 - accuracy: 0.3636\n",
            "Epoch 43/200\n",
            "1/1 - 0s - loss: 1.6526 - accuracy: 0.3636\n",
            "Epoch 44/200\n",
            "1/1 - 0s - loss: 1.6347 - accuracy: 0.3636\n",
            "Epoch 45/200\n",
            "1/1 - 0s - loss: 1.6161 - accuracy: 0.3636\n",
            "Epoch 46/200\n",
            "1/1 - 0s - loss: 1.5969 - accuracy: 0.3636\n",
            "Epoch 47/200\n",
            "1/1 - 0s - loss: 1.5771 - accuracy: 0.3636\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 1.5570 - accuracy: 0.4545\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 1.5366 - accuracy: 0.5455\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 1.5160 - accuracy: 0.6364\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 1.4952 - accuracy: 0.6364\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 1.4745 - accuracy: 0.6364\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 1.4536 - accuracy: 0.6364\n",
            "Epoch 54/200\n",
            "1/1 - 0s - loss: 1.4328 - accuracy: 0.6364\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 1.4120 - accuracy: 0.6364\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 1.3911 - accuracy: 0.6364\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 1.3702 - accuracy: 0.6364\n",
            "Epoch 58/200\n",
            "1/1 - 0s - loss: 1.3492 - accuracy: 0.7273\n",
            "Epoch 59/200\n",
            "1/1 - 0s - loss: 1.3282 - accuracy: 0.7273\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 1.3072 - accuracy: 0.7273\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 1.2862 - accuracy: 0.7273\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 1.2652 - accuracy: 0.7273\n",
            "Epoch 63/200\n",
            "1/1 - 0s - loss: 1.2443 - accuracy: 0.7273\n",
            "Epoch 64/200\n",
            "1/1 - 0s - loss: 1.2235 - accuracy: 0.7273\n",
            "Epoch 65/200\n",
            "1/1 - 0s - loss: 1.2029 - accuracy: 0.7273\n",
            "Epoch 66/200\n",
            "1/1 - 0s - loss: 1.1826 - accuracy: 0.7273\n",
            "Epoch 67/200\n",
            "1/1 - 0s - loss: 1.1627 - accuracy: 0.7273\n",
            "Epoch 68/200\n",
            "1/1 - 0s - loss: 1.1431 - accuracy: 0.7273\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 1.1239 - accuracy: 0.7273\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 1.1051 - accuracy: 0.7273\n",
            "Epoch 71/200\n",
            "1/1 - 0s - loss: 1.0868 - accuracy: 0.7273\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 1.0690 - accuracy: 0.7273\n",
            "Epoch 73/200\n",
            "1/1 - 0s - loss: 1.0515 - accuracy: 0.7273\n",
            "Epoch 74/200\n",
            "1/1 - 0s - loss: 1.0345 - accuracy: 0.7273\n",
            "Epoch 75/200\n",
            "1/1 - 0s - loss: 1.0179 - accuracy: 0.7273\n",
            "Epoch 76/200\n",
            "1/1 - 0s - loss: 1.0016 - accuracy: 0.7273\n",
            "Epoch 77/200\n",
            "1/1 - 0s - loss: 0.9857 - accuracy: 0.7273\n",
            "Epoch 78/200\n",
            "1/1 - 0s - loss: 0.9701 - accuracy: 0.7273\n",
            "Epoch 79/200\n",
            "1/1 - 0s - loss: 0.9548 - accuracy: 0.7273\n",
            "Epoch 80/200\n",
            "1/1 - 0s - loss: 0.9398 - accuracy: 0.7273\n",
            "Epoch 81/200\n",
            "1/1 - 0s - loss: 0.9251 - accuracy: 0.7273\n",
            "Epoch 82/200\n",
            "1/1 - 0s - loss: 0.9107 - accuracy: 0.7273\n",
            "Epoch 83/200\n",
            "1/1 - 0s - loss: 0.8966 - accuracy: 0.7273\n",
            "Epoch 84/200\n",
            "1/1 - 0s - loss: 0.8827 - accuracy: 0.7273\n",
            "Epoch 85/200\n",
            "1/1 - 0s - loss: 0.8691 - accuracy: 0.7273\n",
            "Epoch 86/200\n",
            "1/1 - 0s - loss: 0.8556 - accuracy: 0.7273\n",
            "Epoch 87/200\n",
            "1/1 - 0s - loss: 0.8424 - accuracy: 0.7273\n",
            "Epoch 88/200\n",
            "1/1 - 0s - loss: 0.8294 - accuracy: 0.7273\n",
            "Epoch 89/200\n",
            "1/1 - 0s - loss: 0.8166 - accuracy: 0.7273\n",
            "Epoch 90/200\n",
            "1/1 - 0s - loss: 0.8040 - accuracy: 0.7273\n",
            "Epoch 91/200\n",
            "1/1 - 0s - loss: 0.7916 - accuracy: 0.7273\n",
            "Epoch 92/200\n",
            "1/1 - 0s - loss: 0.7793 - accuracy: 0.7273\n",
            "Epoch 93/200\n",
            "1/1 - 0s - loss: 0.7673 - accuracy: 0.7273\n",
            "Epoch 94/200\n",
            "1/1 - 0s - loss: 0.7554 - accuracy: 0.7273\n",
            "Epoch 95/200\n",
            "1/1 - 0s - loss: 0.7437 - accuracy: 0.7273\n",
            "Epoch 96/200\n",
            "1/1 - 0s - loss: 0.7322 - accuracy: 0.7273\n",
            "Epoch 97/200\n",
            "1/1 - 0s - loss: 0.7209 - accuracy: 0.7273\n",
            "Epoch 98/200\n",
            "1/1 - 0s - loss: 0.7097 - accuracy: 0.7273\n",
            "Epoch 99/200\n",
            "1/1 - 0s - loss: 0.6987 - accuracy: 0.7273\n",
            "Epoch 100/200\n",
            "1/1 - 0s - loss: 0.6878 - accuracy: 0.7273\n",
            "Epoch 101/200\n",
            "1/1 - 0s - loss: 0.6771 - accuracy: 0.7273\n",
            "Epoch 102/200\n",
            "1/1 - 0s - loss: 0.6666 - accuracy: 0.7273\n",
            "Epoch 103/200\n",
            "1/1 - 0s - loss: 0.6562 - accuracy: 0.7273\n",
            "Epoch 104/200\n",
            "1/1 - 0s - loss: 0.6460 - accuracy: 0.7273\n",
            "Epoch 105/200\n",
            "1/1 - 0s - loss: 0.6359 - accuracy: 0.7273\n",
            "Epoch 106/200\n",
            "1/1 - 0s - loss: 0.6259 - accuracy: 0.7273\n",
            "Epoch 107/200\n",
            "1/1 - 0s - loss: 0.6161 - accuracy: 0.7273\n",
            "Epoch 108/200\n",
            "1/1 - 0s - loss: 0.6064 - accuracy: 0.7273\n",
            "Epoch 109/200\n",
            "1/1 - 0s - loss: 0.5968 - accuracy: 0.8182\n",
            "Epoch 110/200\n",
            "1/1 - 0s - loss: 0.5874 - accuracy: 0.8182\n",
            "Epoch 111/200\n",
            "1/1 - 0s - loss: 0.5781 - accuracy: 0.8182\n",
            "Epoch 112/200\n",
            "1/1 - 0s - loss: 0.5689 - accuracy: 0.8182\n",
            "Epoch 113/200\n",
            "1/1 - 0s - loss: 0.5598 - accuracy: 0.8182\n",
            "Epoch 114/200\n",
            "1/1 - 0s - loss: 0.5509 - accuracy: 0.8182\n",
            "Epoch 115/200\n",
            "1/1 - 0s - loss: 0.5420 - accuracy: 0.8182\n",
            "Epoch 116/200\n",
            "1/1 - 0s - loss: 0.5333 - accuracy: 0.9091\n",
            "Epoch 117/200\n",
            "1/1 - 0s - loss: 0.5247 - accuracy: 0.9091\n",
            "Epoch 118/200\n",
            "1/1 - 0s - loss: 0.5162 - accuracy: 0.9091\n",
            "Epoch 119/200\n",
            "1/1 - 0s - loss: 0.5078 - accuracy: 0.9091\n",
            "Epoch 120/200\n",
            "1/1 - 0s - loss: 0.4995 - accuracy: 0.9091\n",
            "Epoch 121/200\n",
            "1/1 - 0s - loss: 0.4913 - accuracy: 0.9091\n",
            "Epoch 122/200\n",
            "1/1 - 0s - loss: 0.4831 - accuracy: 0.9091\n",
            "Epoch 123/200\n",
            "1/1 - 0s - loss: 0.4751 - accuracy: 0.9091\n",
            "Epoch 124/200\n",
            "1/1 - 0s - loss: 0.4672 - accuracy: 0.9091\n",
            "Epoch 125/200\n",
            "1/1 - 0s - loss: 0.4594 - accuracy: 0.9091\n",
            "Epoch 126/200\n",
            "1/1 - 0s - loss: 0.4517 - accuracy: 0.9091\n",
            "Epoch 127/200\n",
            "1/1 - 0s - loss: 0.4440 - accuracy: 0.9091\n",
            "Epoch 128/200\n",
            "1/1 - 0s - loss: 0.4365 - accuracy: 0.9091\n",
            "Epoch 129/200\n",
            "1/1 - 0s - loss: 0.4291 - accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "1/1 - 0s - loss: 0.4217 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "1/1 - 0s - loss: 0.4144 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "1/1 - 0s - loss: 0.4072 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "1/1 - 0s - loss: 0.4001 - accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "1/1 - 0s - loss: 0.3931 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "1/1 - 0s - loss: 0.3862 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "1/1 - 0s - loss: 0.3794 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "1/1 - 0s - loss: 0.3726 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "1/1 - 0s - loss: 0.3660 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "1/1 - 0s - loss: 0.3594 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "1/1 - 0s - loss: 0.3529 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "1/1 - 0s - loss: 0.3465 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "1/1 - 0s - loss: 0.3402 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "1/1 - 0s - loss: 0.3340 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "1/1 - 0s - loss: 0.3279 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "1/1 - 0s - loss: 0.3218 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "1/1 - 0s - loss: 0.3159 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "1/1 - 0s - loss: 0.3100 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "1/1 - 0s - loss: 0.3042 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "1/1 - 0s - loss: 0.2985 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "1/1 - 0s - loss: 0.2929 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "1/1 - 0s - loss: 0.2874 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "1/1 - 0s - loss: 0.2819 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "1/1 - 0s - loss: 0.2766 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "1/1 - 0s - loss: 0.2713 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "1/1 - 0s - loss: 0.2661 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "1/1 - 0s - loss: 0.2610 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "1/1 - 0s - loss: 0.2560 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "1/1 - 0s - loss: 0.2511 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "1/1 - 0s - loss: 0.2462 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "1/1 - 0s - loss: 0.2415 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "1/1 - 0s - loss: 0.2368 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "1/1 - 0s - loss: 0.2322 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "1/1 - 0s - loss: 0.2277 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "1/1 - 0s - loss: 0.2233 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "1/1 - 0s - loss: 0.2189 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "1/1 - 0s - loss: 0.2146 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "1/1 - 0s - loss: 0.2105 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "1/1 - 0s - loss: 0.2064 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "1/1 - 0s - loss: 0.2023 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "1/1 - 0s - loss: 0.1984 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "1/1 - 0s - loss: 0.1945 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "1/1 - 0s - loss: 0.1907 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "1/1 - 0s - loss: 0.1870 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "1/1 - 0s - loss: 0.1834 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "1/1 - 0s - loss: 0.1798 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "1/1 - 0s - loss: 0.1763 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "1/1 - 0s - loss: 0.1729 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "1/1 - 0s - loss: 0.1696 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "1/1 - 0s - loss: 0.1663 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "1/1 - 0s - loss: 0.1631 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "1/1 - 0s - loss: 0.1599 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "1/1 - 0s - loss: 0.1569 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "1/1 - 0s - loss: 0.1539 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "1/1 - 0s - loss: 0.1509 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "1/1 - 0s - loss: 0.1481 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "1/1 - 0s - loss: 0.1453 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "1/1 - 0s - loss: 0.1425 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "1/1 - 0s - loss: 0.1398 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "1/1 - 0s - loss: 0.1372 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "1/1 - 0s - loss: 0.1347 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "1/1 - 0s - loss: 0.1322 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "1/1 - 0s - loss: 0.1297 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "1/1 - 0s - loss: 0.1273 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "1/1 - 0s - loss: 0.1250 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "1/1 - 0s - loss: 0.1227 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "1/1 - 0s - loss: 0.1205 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "1/1 - 0s - loss: 0.1183 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "1/1 - 0s - loss: 0.1162 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "1/1 - 0s - loss: 0.1141 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "1/1 - 0s - loss: 0.1121 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe57ff910b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vzBgPecQ-HE",
        "colab_type": "code",
        "outputId": "c554f004-2ec2-4b6e-d2de-b3afa4acd905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "def sentence_generation(model, t, current_word, n): \n",
        "    init_word = current_word \n",
        "    sentence = ''\n",
        "    for _ in range(n): \n",
        "        encoded = t.texts_to_sequences([current_word])[0] \n",
        "        encoded = pad_sequences([encoded], maxlen=5, padding='pre') \n",
        "        result = model.predict_classes(encoded, verbose=0)\n",
        " \n",
        "        for word, index in t.word_index.items(): \n",
        "            if index == result: \n",
        "                break \n",
        "        current_word = current_word + ' '  + word \n",
        "        \n",
        "        sentence = sentence + ' ' + word \n",
        "        \n",
        "    sentence = init_word + sentence\n",
        "    return sentence\n",
        "\n",
        "\n",
        "print(sentence_generation(model, t, '경마장에', 4))\n",
        "print(sentence_generation(model, t, '가는', 3))\n",
        "print(sentence_generation(model, t, '그의', 4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "경마장에 있는 말이 뛰고 있다\n",
            "가는 말이 고와야 오는\n",
            "그의 말이 법이다 오는 말이\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSTtB2dYSmCA",
        "colab_type": "text"
      },
      "source": [
        "〽️ Need a lot of training \n",
        "\n",
        "〽️ Not that accuracte when printed out "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAAz7APtSuan",
        "colab_type": "text"
      },
      "source": [
        "# **LSTM ANALYSIS**\n",
        "\n",
        "〽️ Many-to-one LSTM \n",
        "\n",
        "〽️ Predicting next word after sequence \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_KC8YrLSzPW",
        "colab_type": "code",
        "outputId": "fc90a7c4-1fb7-44cf-f277-5f6e198ee197",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "import pandas as pd\n",
        "from string import punctuation\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "df=pd.read_csv('ArticlesApril2018.csv')\n",
        "\n",
        "print('열의 개수: ',len(df.columns))\n",
        "print(df.columns) #see columns in df \n",
        "df['headline'].isnull().values.any() #check null values \n",
        "\n",
        "df.head(1) #see first row \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "열의 개수:  15\n",
            "Index(['articleID', 'articleWordCount', 'byline', 'documentType', 'headline',\n",
            "       'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
            "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>articleID</th>\n",
              "      <th>articleWordCount</th>\n",
              "      <th>byline</th>\n",
              "      <th>documentType</th>\n",
              "      <th>headline</th>\n",
              "      <th>keywords</th>\n",
              "      <th>multimedia</th>\n",
              "      <th>newDesk</th>\n",
              "      <th>printPage</th>\n",
              "      <th>pubDate</th>\n",
              "      <th>sectionName</th>\n",
              "      <th>snippet</th>\n",
              "      <th>source</th>\n",
              "      <th>typeOfMaterial</th>\n",
              "      <th>webURL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5adf6684068401528a2aa69b</td>\n",
              "      <td>781</td>\n",
              "      <td>By JOHN BRANCH</td>\n",
              "      <td>article</td>\n",
              "      <td>Former N.F.L. Cheerleaders’ Settlement Offer: ...</td>\n",
              "      <td>['Workplace Hazards and Violations', 'Football...</td>\n",
              "      <td>68</td>\n",
              "      <td>Sports</td>\n",
              "      <td>0</td>\n",
              "      <td>2018-04-24 17:16:49</td>\n",
              "      <td>Pro Football</td>\n",
              "      <td>“I understand that they could meet with us, pa...</td>\n",
              "      <td>The New York Times</td>\n",
              "      <td>News</td>\n",
              "      <td>https://www.nytimes.com/2018/04/24/sports/foot...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  articleID  ...                                             webURL\n",
              "0  5adf6684068401528a2aa69b  ...  https://www.nytimes.com/2018/04/24/sports/foot...\n",
              "\n",
              "[1 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbhrL4QQTQHA",
        "colab_type": "code",
        "outputId": "74ab8e40-f29d-4652-e12d-e4adccbcae61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "#PREPROCESSING \n",
        "\n",
        "#delete data without headline\n",
        "headline = [] \n",
        "headline.extend(list(df.headline.values)) #collect headline info \n",
        "\n",
        "\n",
        "headline = [n for n in headline if n != \"Unknown\"] \n",
        "\n",
        "print('노이즈값 제거 후 샘플의 개수 : {}'.format(len(headline)))\n",
        "headline[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "노이즈값 제거 후 샘플의 개수 : 1214\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Former N.F.L. Cheerleaders’ Settlement Offer: $1 and a Meeting With Goodell',\n",
              " 'E.P.A. to Unveil a New Rule. Its Effect: Less Science in Policymaking.',\n",
              " 'The New Noma, Explained',\n",
              " 'How a Bag of Texas Dirt  Became a Times Tradition',\n",
              " 'Is School a Place for Self-Expression?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjmNHLWnTguU",
        "colab_type": "code",
        "outputId": "bd35cc6b-5d0c-4e7c-ef00-4140ee685e1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "def repreprocessing(s):\n",
        "    s=s.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "    return ''.join(c for c in s if c not in punctuation).lower() # delete punctuation and make lower case \n",
        "\n",
        "text = [repreprocessing(x) for x in headline]\n",
        "text[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['former nfl cheerleaders settlement offer 1 and a meeting with goodell',\n",
              " 'epa to unveil a new rule its effect less science in policymaking',\n",
              " 'the new noma explained',\n",
              " 'how a bag of texas dirt  became a times tradition',\n",
              " 'is school a place for selfexpression']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3LSUbeXTpU7",
        "colab_type": "code",
        "outputId": "30144c3a-90bb-4b73-baac-49bdeb06f35b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "#tockenizing words\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(text)\n",
        "\n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "print('단어 집합의 크기 : %d' % vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 3494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE0SDXPZTtrY",
        "colab_type": "code",
        "outputId": "1054e9dd-b1fd-4cf9-92de-7c1c691a2504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "#incode as num\n",
        "\n",
        "sequences = list()\n",
        "\n",
        "for line in text: \n",
        "    encoded = t.texts_to_sequences([line])[0] \n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "sequences[:11]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[99, 269],\n",
              " [99, 269, 371],\n",
              " [99, 269, 371, 1115],\n",
              " [99, 269, 371, 1115, 582],\n",
              " [99, 269, 371, 1115, 582, 52],\n",
              " [99, 269, 371, 1115, 582, 52, 7],\n",
              " [99, 269, 371, 1115, 582, 52, 7, 2],\n",
              " [99, 269, 371, 1115, 582, 52, 7, 2, 372],\n",
              " [99, 269, 371, 1115, 582, 52, 7, 2, 372, 10],\n",
              " [99, 269, 371, 1115, 582, 52, 7, 2, 372, 10, 1116],\n",
              " [100, 3]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X39j0HrFT0lg",
        "colab_type": "code",
        "outputId": "c2de236b-48f5-4f01-9286-4f5cc5621f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "#make easy access of words by the index by function \n",
        "\n",
        "index_to_word={}\n",
        "for key, value in t.word_index.items(): \n",
        "    index_to_word[value] = key\n",
        "\n",
        "print('빈도수 상위 582번 단어 : {}'.format(index_to_word[582]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "빈도수 상위 582번 단어 : offer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV20sxoBT7RS",
        "colab_type": "code",
        "outputId": "c0bede81-e233-4879-871f-e37ad7f864fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "#padd dataset according to longest sequence \n",
        "\n",
        "max_len=max(len(l) for l in sequences)\n",
        "print('샘플의 최대 길이 : {}'.format(max_len))\n",
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "print(sequences[:3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "샘플의 최대 길이 : 24\n",
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0   99  269]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0   99  269  371]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0   99  269  371 1115]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGTEYFC8UApi",
        "colab_type": "code",
        "outputId": "a4fbbb69-66c3-4e4b-8a74-c6da924f65e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "#split x,y \n",
        "sequences = np.array(sequences)\n",
        "X = sequences[:,:-1]\n",
        "y = sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size) #onehot encoding of y\n",
        "y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iJPO5GxUIYh",
        "colab_type": "code",
        "outputId": "5e5423ca-bfdf-4957-c9d9-69d4106acbab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#MODELING\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "\n",
        "#MODEL\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=max_len-1)) #same as simple RNN \n",
        "\n",
        "model.add(LSTM(128)) #use lstm layer with 128 hidden units \n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=200, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "244/244 - 7s - loss: 7.6426 - accuracy: 0.0250\n",
            "Epoch 2/200\n",
            "244/244 - 7s - loss: 7.1067 - accuracy: 0.0304\n",
            "Epoch 3/200\n",
            "244/244 - 7s - loss: 6.9786 - accuracy: 0.0363\n",
            "Epoch 4/200\n",
            "244/244 - 7s - loss: 6.8557 - accuracy: 0.0408\n",
            "Epoch 5/200\n",
            "244/244 - 7s - loss: 6.7023 - accuracy: 0.0473\n",
            "Epoch 6/200\n",
            "244/244 - 7s - loss: 6.5288 - accuracy: 0.0472\n",
            "Epoch 7/200\n",
            "244/244 - 7s - loss: 6.3340 - accuracy: 0.0545\n",
            "Epoch 8/200\n",
            "244/244 - 7s - loss: 6.1313 - accuracy: 0.0607\n",
            "Epoch 9/200\n",
            "244/244 - 7s - loss: 5.9262 - accuracy: 0.0642\n",
            "Epoch 10/200\n",
            "244/244 - 7s - loss: 5.7320 - accuracy: 0.0674\n",
            "Epoch 11/200\n",
            "244/244 - 7s - loss: 5.5523 - accuracy: 0.0734\n",
            "Epoch 12/200\n",
            "244/244 - 7s - loss: 5.3812 - accuracy: 0.0787\n",
            "Epoch 13/200\n",
            "244/244 - 7s - loss: 5.2210 - accuracy: 0.0830\n",
            "Epoch 14/200\n",
            "244/244 - 7s - loss: 5.0672 - accuracy: 0.0882\n",
            "Epoch 15/200\n",
            "244/244 - 7s - loss: 4.9224 - accuracy: 0.0980\n",
            "Epoch 16/200\n",
            "244/244 - 7s - loss: 4.7836 - accuracy: 0.1089\n",
            "Epoch 17/200\n",
            "244/244 - 7s - loss: 4.6526 - accuracy: 0.1210\n",
            "Epoch 18/200\n",
            "244/244 - 7s - loss: 4.5242 - accuracy: 0.1329\n",
            "Epoch 19/200\n",
            "244/244 - 7s - loss: 4.4003 - accuracy: 0.1487\n",
            "Epoch 20/200\n",
            "244/244 - 7s - loss: 4.2799 - accuracy: 0.1699\n",
            "Epoch 21/200\n",
            "244/244 - 7s - loss: 4.1674 - accuracy: 0.1843\n",
            "Epoch 22/200\n",
            "244/244 - 7s - loss: 4.0572 - accuracy: 0.2013\n",
            "Epoch 23/200\n",
            "244/244 - 7s - loss: 3.9472 - accuracy: 0.2190\n",
            "Epoch 24/200\n",
            "244/244 - 7s - loss: 3.8411 - accuracy: 0.2384\n",
            "Epoch 25/200\n",
            "244/244 - 7s - loss: 3.7408 - accuracy: 0.2531\n",
            "Epoch 26/200\n",
            "244/244 - 7s - loss: 3.6386 - accuracy: 0.2740\n",
            "Epoch 27/200\n",
            "244/244 - 7s - loss: 3.5488 - accuracy: 0.2926\n",
            "Epoch 28/200\n",
            "244/244 - 7s - loss: 3.4529 - accuracy: 0.3076\n",
            "Epoch 29/200\n",
            "244/244 - 7s - loss: 3.3568 - accuracy: 0.3276\n",
            "Epoch 30/200\n",
            "244/244 - 7s - loss: 3.2689 - accuracy: 0.3441\n",
            "Epoch 31/200\n",
            "244/244 - 7s - loss: 3.1806 - accuracy: 0.3632\n",
            "Epoch 32/200\n",
            "244/244 - 7s - loss: 3.0946 - accuracy: 0.3741\n",
            "Epoch 33/200\n",
            "244/244 - 7s - loss: 3.0143 - accuracy: 0.3888\n",
            "Epoch 34/200\n",
            "244/244 - 7s - loss: 2.9359 - accuracy: 0.4073\n",
            "Epoch 35/200\n",
            "244/244 - 7s - loss: 2.8573 - accuracy: 0.4237\n",
            "Epoch 36/200\n",
            "244/244 - 7s - loss: 2.7823 - accuracy: 0.4379\n",
            "Epoch 37/200\n",
            "244/244 - 7s - loss: 2.7119 - accuracy: 0.4517\n",
            "Epoch 38/200\n",
            "244/244 - 7s - loss: 2.6416 - accuracy: 0.4651\n",
            "Epoch 39/200\n",
            "244/244 - 7s - loss: 2.5744 - accuracy: 0.4735\n",
            "Epoch 40/200\n",
            "244/244 - 7s - loss: 2.5075 - accuracy: 0.4897\n",
            "Epoch 41/200\n",
            "244/244 - 7s - loss: 2.4453 - accuracy: 0.5042\n",
            "Epoch 42/200\n",
            "244/244 - 7s - loss: 2.3828 - accuracy: 0.5158\n",
            "Epoch 43/200\n",
            "244/244 - 7s - loss: 2.3225 - accuracy: 0.5274\n",
            "Epoch 44/200\n",
            "244/244 - 7s - loss: 2.2643 - accuracy: 0.5415\n",
            "Epoch 45/200\n",
            "244/244 - 7s - loss: 2.2060 - accuracy: 0.5493\n",
            "Epoch 46/200\n",
            "244/244 - 7s - loss: 2.1519 - accuracy: 0.5641\n",
            "Epoch 47/200\n",
            "244/244 - 7s - loss: 2.1001 - accuracy: 0.5777\n",
            "Epoch 48/200\n",
            "244/244 - 7s - loss: 2.0489 - accuracy: 0.5818\n",
            "Epoch 49/200\n",
            "244/244 - 7s - loss: 1.9957 - accuracy: 0.5969\n",
            "Epoch 50/200\n",
            "244/244 - 7s - loss: 1.9474 - accuracy: 0.6026\n",
            "Epoch 51/200\n",
            "244/244 - 7s - loss: 1.9008 - accuracy: 0.6164\n",
            "Epoch 52/200\n",
            "244/244 - 7s - loss: 1.8541 - accuracy: 0.6259\n",
            "Epoch 53/200\n",
            "244/244 - 7s - loss: 1.8089 - accuracy: 0.6350\n",
            "Epoch 54/200\n",
            "244/244 - 7s - loss: 1.7654 - accuracy: 0.6385\n",
            "Epoch 55/200\n",
            "244/244 - 7s - loss: 1.7213 - accuracy: 0.6496\n",
            "Epoch 56/200\n",
            "244/244 - 7s - loss: 1.6785 - accuracy: 0.6640\n",
            "Epoch 57/200\n",
            "244/244 - 7s - loss: 1.6365 - accuracy: 0.6717\n",
            "Epoch 58/200\n",
            "244/244 - 7s - loss: 1.5998 - accuracy: 0.6782\n",
            "Epoch 59/200\n",
            "244/244 - 7s - loss: 1.5600 - accuracy: 0.6905\n",
            "Epoch 60/200\n",
            "244/244 - 7s - loss: 1.5235 - accuracy: 0.6941\n",
            "Epoch 61/200\n",
            "244/244 - 7s - loss: 1.4860 - accuracy: 0.7033\n",
            "Epoch 62/200\n",
            "244/244 - 7s - loss: 1.4501 - accuracy: 0.7111\n",
            "Epoch 63/200\n",
            "244/244 - 7s - loss: 1.4141 - accuracy: 0.7160\n",
            "Epoch 64/200\n",
            "244/244 - 7s - loss: 1.3803 - accuracy: 0.7291\n",
            "Epoch 65/200\n",
            "244/244 - 7s - loss: 1.3475 - accuracy: 0.7310\n",
            "Epoch 66/200\n",
            "244/244 - 7s - loss: 1.3173 - accuracy: 0.7398\n",
            "Epoch 67/200\n",
            "244/244 - 7s - loss: 1.2844 - accuracy: 0.7470\n",
            "Epoch 68/200\n",
            "244/244 - 7s - loss: 1.2549 - accuracy: 0.7525\n",
            "Epoch 69/200\n",
            "244/244 - 7s - loss: 1.2225 - accuracy: 0.7582\n",
            "Epoch 70/200\n",
            "244/244 - 7s - loss: 1.1925 - accuracy: 0.7657\n",
            "Epoch 71/200\n",
            "244/244 - 7s - loss: 1.1643 - accuracy: 0.7710\n",
            "Epoch 72/200\n",
            "244/244 - 7s - loss: 1.1417 - accuracy: 0.7778\n",
            "Epoch 73/200\n",
            "244/244 - 7s - loss: 1.1131 - accuracy: 0.7839\n",
            "Epoch 74/200\n",
            "244/244 - 7s - loss: 1.0838 - accuracy: 0.7876\n",
            "Epoch 75/200\n",
            "244/244 - 7s - loss: 1.0593 - accuracy: 0.7948\n",
            "Epoch 76/200\n",
            "244/244 - 7s - loss: 1.0318 - accuracy: 0.7997\n",
            "Epoch 77/200\n",
            "244/244 - 7s - loss: 1.0084 - accuracy: 0.8035\n",
            "Epoch 78/200\n",
            "244/244 - 7s - loss: 0.9875 - accuracy: 0.8073\n",
            "Epoch 79/200\n",
            "244/244 - 7s - loss: 0.9629 - accuracy: 0.8165\n",
            "Epoch 80/200\n",
            "244/244 - 7s - loss: 0.9408 - accuracy: 0.8161\n",
            "Epoch 81/200\n",
            "244/244 - 7s - loss: 0.9205 - accuracy: 0.8217\n",
            "Epoch 82/200\n",
            "244/244 - 7s - loss: 0.8983 - accuracy: 0.8248\n",
            "Epoch 83/200\n",
            "244/244 - 7s - loss: 0.8782 - accuracy: 0.8301\n",
            "Epoch 84/200\n",
            "244/244 - 7s - loss: 0.8609 - accuracy: 0.8324\n",
            "Epoch 85/200\n",
            "244/244 - 7s - loss: 0.8394 - accuracy: 0.8331\n",
            "Epoch 86/200\n",
            "244/244 - 7s - loss: 0.8178 - accuracy: 0.8412\n",
            "Epoch 87/200\n",
            "244/244 - 7s - loss: 0.7999 - accuracy: 0.8433\n",
            "Epoch 88/200\n",
            "244/244 - 7s - loss: 0.7843 - accuracy: 0.8475\n",
            "Epoch 89/200\n",
            "244/244 - 7s - loss: 0.7680 - accuracy: 0.8476\n",
            "Epoch 90/200\n",
            "244/244 - 7s - loss: 0.7506 - accuracy: 0.8516\n",
            "Epoch 91/200\n",
            "244/244 - 7s - loss: 0.7344 - accuracy: 0.8552\n",
            "Epoch 92/200\n",
            "244/244 - 7s - loss: 0.7182 - accuracy: 0.8592\n",
            "Epoch 93/200\n",
            "244/244 - 7s - loss: 0.7028 - accuracy: 0.8621\n",
            "Epoch 94/200\n",
            "244/244 - 7s - loss: 0.6898 - accuracy: 0.8633\n",
            "Epoch 95/200\n",
            "244/244 - 7s - loss: 0.6742 - accuracy: 0.8629\n",
            "Epoch 96/200\n",
            "244/244 - 7s - loss: 0.6599 - accuracy: 0.8683\n",
            "Epoch 97/200\n",
            "244/244 - 7s - loss: 0.6470 - accuracy: 0.8708\n",
            "Epoch 98/200\n",
            "244/244 - 7s - loss: 0.6319 - accuracy: 0.8740\n",
            "Epoch 99/200\n",
            "244/244 - 7s - loss: 0.6203 - accuracy: 0.8756\n",
            "Epoch 100/200\n",
            "244/244 - 7s - loss: 0.6096 - accuracy: 0.8777\n",
            "Epoch 101/200\n",
            "244/244 - 7s - loss: 0.5952 - accuracy: 0.8795\n",
            "Epoch 102/200\n",
            "244/244 - 7s - loss: 0.5829 - accuracy: 0.8817\n",
            "Epoch 103/200\n",
            "244/244 - 7s - loss: 0.5706 - accuracy: 0.8862\n",
            "Epoch 104/200\n",
            "244/244 - 7s - loss: 0.5615 - accuracy: 0.8856\n",
            "Epoch 105/200\n",
            "244/244 - 7s - loss: 0.5510 - accuracy: 0.8881\n",
            "Epoch 106/200\n",
            "244/244 - 7s - loss: 0.5400 - accuracy: 0.8891\n",
            "Epoch 107/200\n",
            "244/244 - 7s - loss: 0.5286 - accuracy: 0.8890\n",
            "Epoch 108/200\n",
            "244/244 - 7s - loss: 0.5184 - accuracy: 0.8952\n",
            "Epoch 109/200\n",
            "244/244 - 7s - loss: 0.5106 - accuracy: 0.8944\n",
            "Epoch 110/200\n",
            "244/244 - 7s - loss: 0.5018 - accuracy: 0.8953\n",
            "Epoch 111/200\n",
            "244/244 - 7s - loss: 0.4914 - accuracy: 0.8967\n",
            "Epoch 112/200\n",
            "244/244 - 7s - loss: 0.4833 - accuracy: 0.8977\n",
            "Epoch 113/200\n",
            "244/244 - 7s - loss: 0.4734 - accuracy: 0.8998\n",
            "Epoch 114/200\n",
            "244/244 - 7s - loss: 0.4645 - accuracy: 0.9027\n",
            "Epoch 115/200\n",
            "244/244 - 7s - loss: 0.4572 - accuracy: 0.9011\n",
            "Epoch 116/200\n",
            "244/244 - 7s - loss: 0.4511 - accuracy: 0.9034\n",
            "Epoch 117/200\n",
            "244/244 - 7s - loss: 0.4421 - accuracy: 0.9009\n",
            "Epoch 118/200\n",
            "244/244 - 7s - loss: 0.4355 - accuracy: 0.9040\n",
            "Epoch 119/200\n",
            "244/244 - 7s - loss: 0.4273 - accuracy: 0.9053\n",
            "Epoch 120/200\n",
            "244/244 - 7s - loss: 0.4202 - accuracy: 0.9047\n",
            "Epoch 121/200\n",
            "244/244 - 7s - loss: 0.4145 - accuracy: 0.9067\n",
            "Epoch 122/200\n",
            "244/244 - 7s - loss: 0.4068 - accuracy: 0.9068\n",
            "Epoch 123/200\n",
            "244/244 - 7s - loss: 0.4016 - accuracy: 0.9095\n",
            "Epoch 124/200\n",
            "244/244 - 7s - loss: 0.3963 - accuracy: 0.9084\n",
            "Epoch 125/200\n",
            "244/244 - 7s - loss: 0.3936 - accuracy: 0.9116\n",
            "Epoch 126/200\n",
            "244/244 - 7s - loss: 0.3862 - accuracy: 0.9111\n",
            "Epoch 127/200\n",
            "244/244 - 7s - loss: 0.3787 - accuracy: 0.9114\n",
            "Epoch 128/200\n",
            "244/244 - 7s - loss: 0.3740 - accuracy: 0.9126\n",
            "Epoch 129/200\n",
            "244/244 - 8s - loss: 0.3708 - accuracy: 0.9116\n",
            "Epoch 130/200\n",
            "244/244 - 10s - loss: 0.3638 - accuracy: 0.9130\n",
            "Epoch 131/200\n",
            "244/244 - 7s - loss: 0.3603 - accuracy: 0.9123\n",
            "Epoch 132/200\n",
            "244/244 - 7s - loss: 0.3581 - accuracy: 0.9144\n",
            "Epoch 133/200\n",
            "244/244 - 7s - loss: 0.3520 - accuracy: 0.9159\n",
            "Epoch 134/200\n",
            "244/244 - 7s - loss: 0.3572 - accuracy: 0.9129\n",
            "Epoch 135/200\n",
            "244/244 - 7s - loss: 0.3466 - accuracy: 0.9146\n",
            "Epoch 136/200\n",
            "244/244 - 7s - loss: 0.3405 - accuracy: 0.9148\n",
            "Epoch 137/200\n",
            "244/244 - 7s - loss: 0.3386 - accuracy: 0.9148\n",
            "Epoch 138/200\n",
            "244/244 - 7s - loss: 0.3346 - accuracy: 0.9149\n",
            "Epoch 139/200\n",
            "244/244 - 7s - loss: 0.3312 - accuracy: 0.9157\n",
            "Epoch 140/200\n",
            "244/244 - 7s - loss: 0.3279 - accuracy: 0.9164\n",
            "Epoch 141/200\n",
            "244/244 - 7s - loss: 0.3252 - accuracy: 0.9150\n",
            "Epoch 142/200\n",
            "244/244 - 7s - loss: 0.3235 - accuracy: 0.9166\n",
            "Epoch 143/200\n",
            "244/244 - 7s - loss: 0.3188 - accuracy: 0.9155\n",
            "Epoch 144/200\n",
            "244/244 - 7s - loss: 0.3156 - accuracy: 0.9162\n",
            "Epoch 145/200\n",
            "244/244 - 7s - loss: 0.3139 - accuracy: 0.9162\n",
            "Epoch 146/200\n",
            "244/244 - 7s - loss: 0.3109 - accuracy: 0.9155\n",
            "Epoch 147/200\n",
            "244/244 - 7s - loss: 0.3096 - accuracy: 0.9139\n",
            "Epoch 148/200\n",
            "244/244 - 7s - loss: 0.3083 - accuracy: 0.9166\n",
            "Epoch 149/200\n",
            "244/244 - 7s - loss: 0.3168 - accuracy: 0.9153\n",
            "Epoch 150/200\n",
            "244/244 - 7s - loss: 0.3121 - accuracy: 0.9177\n",
            "Epoch 151/200\n",
            "244/244 - 7s - loss: 0.3024 - accuracy: 0.9161\n",
            "Epoch 152/200\n",
            "244/244 - 7s - loss: 0.3000 - accuracy: 0.9181\n",
            "Epoch 153/200\n",
            "244/244 - 7s - loss: 0.2973 - accuracy: 0.9153\n",
            "Epoch 154/200\n",
            "244/244 - 7s - loss: 0.2965 - accuracy: 0.9170\n",
            "Epoch 155/200\n",
            "244/244 - 7s - loss: 0.2942 - accuracy: 0.9167\n",
            "Epoch 156/200\n",
            "244/244 - 7s - loss: 0.2928 - accuracy: 0.9171\n",
            "Epoch 157/200\n",
            "244/244 - 7s - loss: 0.2928 - accuracy: 0.9163\n",
            "Epoch 158/200\n",
            "244/244 - 7s - loss: 0.2904 - accuracy: 0.9157\n",
            "Epoch 159/200\n",
            "244/244 - 7s - loss: 0.2925 - accuracy: 0.9171\n",
            "Epoch 160/200\n",
            "244/244 - 7s - loss: 0.2881 - accuracy: 0.9163\n",
            "Epoch 161/200\n",
            "244/244 - 7s - loss: 0.2868 - accuracy: 0.9186\n",
            "Epoch 162/200\n",
            "244/244 - 7s - loss: 0.2849 - accuracy: 0.9158\n",
            "Epoch 163/200\n",
            "244/244 - 7s - loss: 0.2849 - accuracy: 0.9176\n",
            "Epoch 164/200\n",
            "244/244 - 7s - loss: 0.2829 - accuracy: 0.9175\n",
            "Epoch 165/200\n",
            "244/244 - 7s - loss: 0.2814 - accuracy: 0.9181\n",
            "Epoch 166/200\n",
            "244/244 - 7s - loss: 0.2814 - accuracy: 0.9162\n",
            "Epoch 167/200\n",
            "244/244 - 7s - loss: 0.2818 - accuracy: 0.9146\n",
            "Epoch 168/200\n",
            "244/244 - 7s - loss: 0.3110 - accuracy: 0.9099\n",
            "Epoch 169/200\n",
            "244/244 - 7s - loss: 0.3038 - accuracy: 0.9125\n",
            "Epoch 170/200\n",
            "244/244 - 7s - loss: 0.2805 - accuracy: 0.9163\n",
            "Epoch 171/200\n",
            "244/244 - 7s - loss: 0.2784 - accuracy: 0.9161\n",
            "Epoch 172/200\n",
            "244/244 - 7s - loss: 0.2758 - accuracy: 0.9164\n",
            "Epoch 173/200\n",
            "244/244 - 7s - loss: 0.2742 - accuracy: 0.9171\n",
            "Epoch 174/200\n",
            "244/244 - 7s - loss: 0.2735 - accuracy: 0.9171\n",
            "Epoch 175/200\n",
            "244/244 - 7s - loss: 0.2730 - accuracy: 0.9175\n",
            "Epoch 176/200\n",
            "244/244 - 7s - loss: 0.2736 - accuracy: 0.9157\n",
            "Epoch 177/200\n",
            "244/244 - 7s - loss: 0.2725 - accuracy: 0.9163\n",
            "Epoch 178/200\n",
            "244/244 - 7s - loss: 0.2725 - accuracy: 0.9168\n",
            "Epoch 179/200\n",
            "244/244 - 7s - loss: 0.2720 - accuracy: 0.9157\n",
            "Epoch 180/200\n",
            "244/244 - 7s - loss: 0.2706 - accuracy: 0.9172\n",
            "Epoch 181/200\n",
            "244/244 - 7s - loss: 0.2701 - accuracy: 0.9173\n",
            "Epoch 182/200\n",
            "244/244 - 7s - loss: 0.2709 - accuracy: 0.9172\n",
            "Epoch 183/200\n",
            "244/244 - 7s - loss: 0.2723 - accuracy: 0.9164\n",
            "Epoch 184/200\n",
            "244/244 - 7s - loss: 0.2787 - accuracy: 0.9163\n",
            "Epoch 185/200\n",
            "244/244 - 7s - loss: 0.2731 - accuracy: 0.9158\n",
            "Epoch 186/200\n",
            "244/244 - 7s - loss: 0.2698 - accuracy: 0.9150\n",
            "Epoch 187/200\n",
            "244/244 - 7s - loss: 0.2677 - accuracy: 0.9171\n",
            "Epoch 188/200\n",
            "244/244 - 7s - loss: 0.2669 - accuracy: 0.9162\n",
            "Epoch 189/200\n",
            "244/244 - 7s - loss: 0.2663 - accuracy: 0.9162\n",
            "Epoch 190/200\n",
            "244/244 - 7s - loss: 0.2659 - accuracy: 0.9164\n",
            "Epoch 191/200\n",
            "244/244 - 7s - loss: 0.2663 - accuracy: 0.9161\n",
            "Epoch 192/200\n",
            "244/244 - 7s - loss: 0.2665 - accuracy: 0.9157\n",
            "Epoch 193/200\n",
            "244/244 - 7s - loss: 0.2655 - accuracy: 0.9172\n",
            "Epoch 194/200\n",
            "244/244 - 7s - loss: 0.2651 - accuracy: 0.9175\n",
            "Epoch 195/200\n",
            "244/244 - 7s - loss: 0.2663 - accuracy: 0.9180\n",
            "Epoch 196/200\n",
            "244/244 - 7s - loss: 0.2729 - accuracy: 0.9154\n",
            "Epoch 197/200\n",
            "244/244 - 7s - loss: 0.2774 - accuracy: 0.9149\n",
            "Epoch 198/200\n",
            "244/244 - 7s - loss: 0.2655 - accuracy: 0.9170\n",
            "Epoch 199/200\n",
            "244/244 - 7s - loss: 0.2634 - accuracy: 0.9157\n",
            "Epoch 200/200\n",
            "244/244 - 7s - loss: 0.2613 - accuracy: 0.9164\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe57bb509e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmN2dInTUSXT",
        "colab_type": "code",
        "outputId": "6fa8727d-fe56-47bb-fff1-1c809354882d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "def sentence_generation(model, t, current_word, n): \n",
        "  # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
        "    init_word = current_word \n",
        "    \n",
        "    sentence = ''\n",
        "    for _ in range(n): \n",
        "      \n",
        "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
        "\n",
        "        encoded = pad_sequences([encoded], maxlen=23, padding='pre') # 데이터에 대한 패딩\n",
        "        result = model.predict_classes(encoded, verbose=0)\n",
        "    \n",
        "    # 입력한 X(현재 단어)에 대해서 y를 예측하고 y(예측한 단어)를 result에 저장.\n",
        "        for word, index in t.word_index.items(): \n",
        "\n",
        "            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
        "\n",
        "                break # 해당 단어가 예측 단어이므로 break\n",
        "\n",
        "        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
        "        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장\n",
        "   \n",
        "   \n",
        "    sentence = init_word + sentence\n",
        "    return sentence\n",
        "\n",
        "print(sentence_generation(model, t, 'i', 20))\n",
        "print(sentence_generation(model, t, 'how', 20))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i disapprove of school vouchers can i still apply for them in fears over weapons too columbia we my them too\n",
            "how to make facebook more accountable as china triumph of contradiction ousted against we we risks do it human life spreads\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQVuQJgIUziR",
        "colab_type": "text"
      },
      "source": [
        "# **Tagging Task using Keras**\n",
        "\n",
        "〽️ Many-to-many RNN \n",
        "\n",
        "〽️ Uses Bidirectional RNN-LSTM (seeing later input data too) \n",
        "\n",
        "〽️ Predicting the tag of the input \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9Sjadm8VUWU",
        "colab_type": "code",
        "outputId": "ec219fbb-aac8-423c-bf7f-d12a6033b341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "import re\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "f = open('train.txt', 'r')\n",
        "tagged_sentences = []\n",
        "sentence = []\n",
        "\n",
        "#read the dataset by reading lines \n",
        "\n",
        "for line in f:\n",
        "    if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
        "        if len(sentence) > 0:\n",
        "            tagged_sentences.append(sentence)\n",
        "            sentence = []\n",
        "        continue\n",
        "    splits = line.split(' ') # split words according to sapce \n",
        "    splits[-1] = re.sub(r'\\n', '', splits[-1]) # delete \\n\n",
        "    word = splits[0].lower() #make lower \n",
        "    sentence.append([word, splits[-1]]) \n",
        "\n",
        "print(tagged_sentences[0]) #trainset is sequences with tag information included "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['eu', 'B-ORG'], ['rejects', 'O'], ['german', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['british', 'B-MISC'], ['lamb', 'O'], ['.', 'O']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMvHYHlMWDrY",
        "colab_type": "code",
        "outputId": "68981101-58bd-456d-f608-4f2b3172fa19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "sentences, ner_tags = [], [] \n",
        "\n",
        "for tagged_sentence in tagged_sentences:\n",
        "\n",
        "    sentence, tag_info = zip(*tagged_sentence) #save/split the data into sentence and tag_info \n",
        "    sentences.append(list(sentence)) #save only the words into sentences list \n",
        "    ner_tags.append(list(tag_info)) #save the tag info into the ner_tag list \n",
        "\n",
        "\n",
        "#all words, tags are saved to seperate lists \n",
        "print(sentences[0])\n",
        "print(ner_tags[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
            "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJIu1g9LWhQF",
        "colab_type": "code",
        "outputId": "0fbb035f-6509-4f3e-b3a0-3c48b925f8b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "#eda of dataset \n",
        "\n",
        "print('샘플의 최대 길이 : %d' % max(len(l) for l in sentences))\n",
        "print('샘플의 평균 길이 : %f' % (sum(map(len, sentences))/len(sentences)))\n",
        "plt.hist([len(s) for s in sentences], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "샘플의 최대 길이 : 113\n",
            "샘플의 평균 길이 : 14.502635\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enZ0XWG9WoCH",
        "colab_type": "code",
        "outputId": "0d366ae2-f137-47af-d00f-8f011fff3f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "max_words = 4000\n",
        "\n",
        "#tokenize word\n",
        "src_tokenizer = Tokenizer(num_words=max_words, oov_token='OOV') #use only top 4000 words \n",
        "src_tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "#tokenzie tag\n",
        "tar_tokenizer = Tokenizer()\n",
        "tar_tokenizer.fit_on_texts(ner_tags)\n",
        "\n",
        "vocab_size = max_words\n",
        "tag_size = len(tar_tokenizer.word_index) + 1\n",
        "print('단어 집합의 크기 : {}'.format(vocab_size))\n",
        "print('개체명 태깅 정보 집합의 크기 : {}'.format(tag_size))\n",
        "\n",
        "X_train = src_tokenizer.texts_to_sequences(sentences)\n",
        "y_train = tar_tokenizer.texts_to_sequences(ner_tags)\n",
        "\n",
        "print(X_train[0])\n",
        "print(y_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 4000\n",
            "개체명 태깅 정보 집합의 크기 : 10\n",
            "[989, 1, 205, 629, 7, 3939, 216, 1, 3]\n",
            "[4, 1, 7, 1, 1, 1, 7, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZWfJEluW9PT",
        "colab_type": "code",
        "outputId": "a3c9622a-b4bb-46a8-9ba3-1a33ce459ee1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "\n",
        "#for easier access make sure to re-produce the words from index\n",
        "\n",
        "index_to_word = src_tokenizer.index_word\n",
        "index_to_ner = tar_tokenizer.index_word\n",
        "\n",
        "#example for decoding the words \n",
        "decoded = []\n",
        "for index in X_train[0] : #only for the first row \n",
        "    decoded.append(index_to_word[index]) #index->words\n",
        "\n",
        "print('기존 문장 : {}'.format(sentences[0]))\n",
        "print('빈도수가 낮은 단어가 OOV 처리된 문장 : {}'.format(decoded))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "기존 문장 : ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
            "빈도수가 낮은 단어가 OOV 처리된 문장 : ['eu', 'OOV', 'german', 'call', 'to', 'boycott', 'british', 'OOV', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd_jAlyaXNGK",
        "colab_type": "code",
        "outputId": "e7f90720-43f7-4f40-bf18-252ef44cff19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "max_len = 70\n",
        "#only use 70 for length \n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
        "\n",
        "y_train = pad_sequences(y_train, padding='post', maxlen=max_len)\n",
        "\n",
        "#split dataset \n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=.2, random_state=12)\n",
        "y_train = to_categorical(y_train, num_classes=tag_size)\n",
        "y_test = to_categorical(y_test, num_classes=tag_size)\n",
        "\n",
        "print('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\n",
        "print('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\n",
        "print('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\n",
        "print('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 샘플 문장의 크기 : (11232, 70)\n",
            "훈련 샘플 레이블의 크기 : (11232, 70, 10)\n",
            "테스트 샘플 문장의 크기 : (2808, 70)\n",
            "테스트 샘플 레이블의 크기 : (2808, 70, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_j-CbQ_XaZo",
        "colab_type": "code",
        "outputId": "fd95fda5-6c5e-4f59-89a1-f5f3179d98b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "source": [
        "#Modeling\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Bidirectional, TimeDistributed\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len, mask_zero=True))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True))) #use bidirectional lstm\n",
        "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=128, epochs=3,  validation_data=(X_test, y_test)) #less training -> less epochs \n",
        "\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 11232 samples, validate on 2808 samples\n",
            "Epoch 1/3\n",
            "11232/11232 [==============================] - 146s 13ms/step - loss: 0.1848 - accuracy: 0.8241 - val_loss: 0.1326 - val_accuracy: 0.8324\n",
            "Epoch 2/3\n",
            "11232/11232 [==============================] - 145s 13ms/step - loss: 0.0981 - accuracy: 0.8572 - val_loss: 0.0783 - val_accuracy: 0.8888\n",
            "Epoch 3/3\n",
            "11232/11232 [==============================] - 145s 13ms/step - loss: 0.0628 - accuracy: 0.9087 - val_loss: 0.0545 - val_accuracy: 0.9269\n",
            "2808/2808 [==============================] - 13s 5ms/step\n",
            "\n",
            " 테스트 정확도: 0.9269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDiNyeLOXld5",
        "colab_type": "code",
        "outputId": "e116fc38-e7bb-4979-b6bf-30e248c57976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "#try on specific data : easy to visualzie \n",
        "\n",
        "i=10 #try out by prinitng out the words \n",
        "y_predicted = model.predict(np.array([X_test[i]])) # get the predicted y values \n",
        "\n",
        "y_predicted = np.argmax(y_predicted, axis=-1) #one-hot encoding -> integer \n",
        "true = np.argmax(y_test[i], -1) \n",
        "\n",
        "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
        "print(35 * \"-\")\n",
        "\n",
        "for w, t, pred in zip(X_test[i], true, y_predicted[0]):\n",
        "    if w != 0:\n",
        "        print(\"{:17}: {:7} {}\".format(index_to_word[w], index_to_ner[t].upper(), index_to_ner[pred].upper()))\n",
        "\n",
        "\n",
        "#not that good because not a lot of training occured "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어             |실제값  |예측값\n",
            "-----------------------------------\n",
            "thai             : B-MISC  B-MISC\n",
            "official         : O       O\n",
            "OOV              : O       B-ORG\n",
            "hong             : B-LOC   B-LOC\n",
            "kong             : I-LOC   I-ORG\n",
            "after            : O       O\n",
            "OOV              : O       O\n",
            "OOV              : O       O\n",
            ".                : O       O\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}