{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Weekly13_Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JelYrTIgLi1q",
        "colab_type": "text"
      },
      "source": [
        "# **EN Word2Vec**\n",
        "\n",
        "〽️ Using xml file : needs preprocessing such as deleting tags (/content) \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se6p6Z6RLe9L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c26a5909-090a-433e-97b8-0219a20dc841"
      },
      "source": [
        "import re\n",
        "from lxml import etree\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "# nltk.download('all')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOc0JseHMLje",
        "colab_type": "text"
      },
      "source": [
        "**(1) Loading Data with url**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3eqc7WFLtRt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a2eab6e5-d95f-4258-f688-c8ce227652e9"
      },
      "source": [
        "#download dataset by url \n",
        "urllib.request.urlretrieve(\"https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip\", filename=\"ted_en-20160408.zip\") \n",
        "\n",
        "\n",
        "with zipfile.ZipFile('ted_en-20160408.zip', 'r') as z:\n",
        "\n",
        "  target_text = etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
        "\n",
        "  parse_text = '\\n'.join(target_text.xpath('//content/text()'))  #save only words inside content tag \n",
        "\n",
        "parse_text[:300]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\\nTo me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation. Both are necessary, but it can be too much of a good thing.\\nConsider Facit\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ub_uJkYMmK3",
        "colab_type": "text"
      },
      "source": [
        "**(2) Preprocessing Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSNSOdyFMqBa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "8f340dd1-9884-48b9-c914-acb631dc913d"
      },
      "source": [
        "\n",
        "#1. delete background audio, laughter.... which has parenthesis \n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "#tokenize the sentences  \n",
        "sent_text=sent_tokenize(content_text)\n",
        "\n",
        "\n",
        "#delete non word, non number info, and make words to lower \n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower()) \n",
        "\n",
        "     normalized_text.append(tokens)\n",
        "     \n",
        "#tokenize the words \n",
        "result = [word_tokenize(sentence) for sentence in normalized_text]\n",
        "\n",
        "for line in result[:10]: \n",
        "    print(line) #visualize the result data :: how data looks like \n",
        "    #seperated with sentences -> tokenized words  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
            "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
            "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n",
            "['consider', 'facit']\n",
            "['i', 'm', 'actually', 'old', 'enough', 'to', 'remember', 'them']\n",
            "['facit', 'was', 'a', 'fantastic', 'company']\n",
            "['they', 'were', 'born', 'deep', 'in', 'the', 'swedish', 'forest', 'and', 'they', 'made', 'the', 'best', 'mechanical', 'calculators', 'in', 'the', 'world']\n",
            "['everybody', 'used', 'them']\n",
            "['and', 'what', 'did', 'facit', 'do', 'when', 'the', 'electronic', 'calculator', 'came', 'along']\n",
            "['they', 'continued', 'doing', 'exactly', 'the', 'same']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXyaik2ONYA0",
        "colab_type": "text"
      },
      "source": [
        "**(3) Training the dataset with Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0-P3X2iMrOY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a4486db6-12a2-4f63-aeb1-240ce46beee0"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "#use word2vec to train the dataset \n",
        "\n",
        "model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)\n",
        "#sentences = tokenized word sentence \n",
        "#size = embedding vector size \n",
        "#windows = how many words we are going to see front, back \n",
        "#min_count = if under 5, lose the word \n",
        "#workers= processer num\n",
        "#sd = 0 : CBOW \n",
        "#sd =1 : skip gram\n",
        "\n",
        "\n",
        "model_result = model.wv.most_similar(\"woman\")\n",
        "print(model_result)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('girl', 0.8535995483398438), ('man', 0.8482182025909424), ('boy', 0.805486798286438), ('lady', 0.7835748195648193), ('child', 0.7515976428985596), ('kid', 0.7249900102615356), ('nurse', 0.7235825657844543), ('soldier', 0.7031956911087036), ('guy', 0.700143575668335), ('gentleman', 0.6947692632675171)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEOgamNrOCuL",
        "colab_type": "text"
      },
      "source": [
        "**(4) Saving the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMYVX2PjOBzb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "56033ace-2aa7-4c9f-fade-5f3d60f10b10"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model.wv.save_word2vec_format('eng_w2v') # save by eng_w2v name \n",
        "\n",
        "loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\")  #load by using this function \n",
        "\n",
        "#can print out same result as above \n",
        "model_result = model.wv.most_similar(\"woman\")\n",
        "print(model_result)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[('girl', 0.8535995483398438), ('man', 0.8482182025909424), ('boy', 0.805486798286438), ('lady', 0.7835748195648193), ('child', 0.7515976428985596), ('kid', 0.7249900102615356), ('nurse', 0.7235825657844543), ('soldier', 0.7031956911087036), ('guy', 0.700143575668335), ('gentleman', 0.6947692632675171)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDWzxACOORhQ",
        "colab_type": "text"
      },
      "source": [
        "# **KOREAN Word2Vec**\n",
        "\n",
        "〽️ Using naver movie reviews\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3ilvGXxOVYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "from konlpy.tag import Okt\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# !pip install konlpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE1odhynOfKt",
        "colab_type": "text"
      },
      "source": [
        "**(1) Loading Data with url**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRJZgNf3OW3Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "e25abb88-a811-4b1f-bafc-46e3b6b2b3ab"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
        "train_data = pd.read_table('ratings.txt')\n",
        "train_data = train_data.dropna(how = 'any') # delete null value row \n",
        "train_data[:10] "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8112052</td>\n",
              "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8132799</td>\n",
              "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4655635</td>\n",
              "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9251303</td>\n",
              "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10067386</td>\n",
              "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2190435</td>\n",
              "      <td>사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>9279041</td>\n",
              "      <td>완전 감동입니다 다시봐도 감동</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7865729</td>\n",
              "      <td>개들의 전쟁2 나오나요? 나오면 1빠로 보고 싶음</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7477618</td>\n",
              "      <td>굿</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9250537</td>\n",
              "      <td>바보가 아니라 병 쉰 인듯</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                           document  label\n",
              "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
              "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
              "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
              "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
              "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1\n",
              "5   2190435                      사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화      1\n",
              "6   9279041                                   완전 감동입니다 다시봐도 감동      1\n",
              "7   7865729                        개들의 전쟁2 나오나요? 나오면 1빠로 보고 싶음      1\n",
              "8   7477618                                                  굿      1\n",
              "9   9250537                                     바보가 아니라 병 쉰 인듯      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNNUYE-zPCbX",
        "colab_type": "text"
      },
      "source": [
        "**(2) Preprocessing Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLXUQnveOj3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#delete other words than korean \n",
        "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") \n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다'] #deleting stop words\n",
        "\n",
        "okt = Okt() #korean word tokenizer \n",
        "tokenized_data = []\n",
        "\n",
        "\n",
        "for sentence in train_data['document']:\n",
        "\n",
        "    temp_X = okt.morphs(sentence, stem=True) # tokenize\n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # deleting stop words \n",
        "    tokenized_data.append(temp_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDRJzzwGO_y2",
        "colab_type": "text"
      },
      "source": [
        "**(3) Training the dataset with Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KTgopNFO93w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a8dcfd7-ca73-4705-bfce-5477cc474316"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(sentences = tokenized_data, size = 100, window = 5, min_count = 5, workers = 4, sg = 1) #use skip-gram this time \n",
        "model.wv.vectors.shape \n",
        "#the size was 100, so embedded matrix has shape of (16477, 100)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16477, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7kAmea9PN8T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f69ee0d1-e36a-4712-c360-9ec2f7df76d8"
      },
      "source": [
        "#look through model \n",
        "\n",
        "print(model.wv.most_similar(\"김수현\"))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('김은숙', 0.8348084688186646), ('노희경', 0.8332891464233398), ('강지환', 0.8175852298736572), ('류덕환', 0.8153785467147827), ('이민호', 0.8136894106864929), ('차인표', 0.8123669624328613), ('최수종', 0.810583770275116), ('한석규', 0.8068250417709351), ('찰떡', 0.8067680597305298), ('이상우', 0.8042839765548706)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F1aVinFRkvi",
        "colab_type": "text"
      },
      "source": [
        "# **TRAINING word2vec**\n",
        "\n",
        "〽️ two ways of using training : \n",
        "\n",
        "1) train own data by word embedding \n",
        "\n",
        "2) use pretrained embedding vector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bHFPYUmSPnA",
        "colab_type": "text"
      },
      "source": [
        "**Train by embedding layer vectors**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZpfThaRURIH",
        "colab_type": "text"
      },
      "source": [
        "(1) tokenize dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7Ug9xGGSYGj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "a1faf490-febc-4a98-e23c-87540697e37b"
      },
      "source": [
        "#embedding layer \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
        "y_train = [1, 0, 0, 1, 1, 0, 1] #sentimental analysis 1-pos, 0-neg \n",
        "\n",
        "#1 tokenize each sentence \n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(sentences)\n",
        "\n",
        "\n",
        "#get the total vocab size \n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "print(vocab_size)\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhPUKNxnUWMI",
        "colab_type": "text"
      },
      "source": [
        "(2) integer encoding : make them into nums "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c4Z8BGDTV3Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "d1a3f9e7-ac5b-4025-bc0d-263a8a1a930a"
      },
      "source": [
        "\n",
        "#int encoding (make the words into integers)\n",
        "X_encoded = t.texts_to_sequences(sentences)\n",
        "print(X_encoded)\n",
        "\n",
        "\n",
        "#get the max len :: for padding \n",
        "max_len=max(len(l) for l in X_encoded)\n",
        "print(max_len)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P--piQfsUZ5D",
        "colab_type": "text"
      },
      "source": [
        "(3) pad according to longest sentence length which is 4 (all to 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7G821JxTVTL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "ccf8b0ba-2c63-4196-e2ff-90e6f7c61eea"
      },
      "source": [
        "#padd as the max len \n",
        "\n",
        "X_train=pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
        "y_train=np.array(y_train)\n",
        "print(X_train)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1  2  3  4]\n",
            " [ 5  6  0  0]\n",
            " [ 7  8  0  0]\n",
            " [ 9 10  0  0]\n",
            " [11 12  0  0]\n",
            " [13  0  0  0]\n",
            " [14 15  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy4r6nf9UfEm",
        "colab_type": "text"
      },
      "source": [
        "(4) train dataset using embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_o4P0vmTsLo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "abbd3242-ea5f-4fc4-d15c-ef59b4aacda3"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 4, input_length=max_len)) # 모든 임베딩 벡터는 4차원.\n",
        "\n",
        "#EMBEDDING LAYER needs......\n",
        "\n",
        "#vocab_size : length of total word vocab (vocabulary amount)\n",
        "#output_dim : embedding vector dimension \n",
        "#input_length: input sequence ; max length of words in one sentence (every sentence needs to be padded )\n",
        "\n",
        "\n",
        "model.add(Flatten()) # to put in dense layer flatten to 1-dimension \n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "#train dataset \n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 0s - loss: 0.6986 - acc: 0.5714\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 0.6972 - acc: 0.5714\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 0.6959 - acc: 0.5714\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 0.6946 - acc: 0.5714\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 0.6933 - acc: 0.5714\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 0.6921 - acc: 0.5714\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 0.6908 - acc: 0.5714\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 0.6895 - acc: 0.5714\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 0.6882 - acc: 0.5714\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 0.6869 - acc: 0.5714\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 0.6856 - acc: 0.5714\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 0.6843 - acc: 0.5714\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 0.6831 - acc: 0.5714\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 0.6818 - acc: 0.5714\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 0.6805 - acc: 0.5714\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 0.6792 - acc: 0.5714\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 0.6779 - acc: 0.7143\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 0.6766 - acc: 0.7143\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 0.6753 - acc: 0.7143\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 0.6740 - acc: 0.7143\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 0.6727 - acc: 0.7143\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 0.6713 - acc: 0.8571\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 0.6700 - acc: 0.8571\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 0.6687 - acc: 0.8571\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 0.6673 - acc: 0.8571\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 0.6660 - acc: 0.8571\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 0.6647 - acc: 0.8571\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 0.6633 - acc: 0.8571\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 0.6619 - acc: 0.8571\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 0.6606 - acc: 1.0000\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 0.6592 - acc: 1.0000\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 0.6578 - acc: 1.0000\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 0.6564 - acc: 1.0000\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 0.6550 - acc: 1.0000\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 0.6536 - acc: 1.0000\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 0.6522 - acc: 1.0000\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 0.6508 - acc: 1.0000\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 0.6493 - acc: 1.0000\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 0.6479 - acc: 1.0000\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 0.6464 - acc: 1.0000\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 0.6449 - acc: 1.0000\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.6435 - acc: 1.0000\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.6420 - acc: 1.0000\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.6405 - acc: 1.0000\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.6390 - acc: 1.0000\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.6374 - acc: 1.0000\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.6359 - acc: 1.0000\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.6344 - acc: 1.0000\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.6328 - acc: 1.0000\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.6312 - acc: 1.0000\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.6296 - acc: 1.0000\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.6281 - acc: 1.0000\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.6265 - acc: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.6248 - acc: 1.0000\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.6232 - acc: 1.0000\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.6216 - acc: 1.0000\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.6199 - acc: 1.0000\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.6183 - acc: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.6166 - acc: 1.0000\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.6149 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.6132 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.6115 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.6098 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.6081 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.6064 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.6046 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.6029 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.6011 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.5993 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.5975 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.5957 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.5939 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.5921 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.5902 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.5884 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.5865 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.5847 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.5828 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.5809 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.5790 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.5771 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.5752 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.5733 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.5714 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.5694 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.5675 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.5655 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.5636 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.5616 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.5596 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.5576 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.5556 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.5536 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.5516 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.5496 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.5476 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.5455 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.5435 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.5414 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.5394 - acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f97c7f4f048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv3U9WE2PfeG",
        "colab_type": "text"
      },
      "source": [
        "**USING pre-trained wordvectors**\n",
        "\n",
        "〽️ example of English, Korean version \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi96uyhkPvuM",
        "colab_type": "text"
      },
      "source": [
        "**Load pretrained wordvector (1)ENGLISH (2)KOREAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcPAfQ8ePrEc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "95bf0707-56c3-4fd9-ab00-c9c71d4e1944"
      },
      "source": [
        "#english version -- too big \n",
        "import gensim\n",
        "\n",
        "#use google's pretrained wordvector \n",
        "!wget \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)  \n",
        "print(word2vec_model.vectors.shape) #(3000000, 300)\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-01 10:17:24--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.93.77\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.93.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  90.3MB/s    in 17s     \n",
            "\n",
            "2020-06-01 10:17:42 (91.0 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIhZkWIJYsel",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "e12010a6-07d8-4f93-e822-a891e3a1a07b"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 300))#make matrix according to the size of pretrained vector matrix \n",
        "#since the shape has 300 columns , make a zero matrix with this amount \n",
        "np.shape(embedding_matrix)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igRWomrlYzw7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "40c65553-5cc8-4dda-a7be-25a2e8c41401"
      },
      "source": [
        "#function for checking if the word is in the vector matrix \n",
        "\n",
        "def get_vector(word):\n",
        "    if word in word2vec_model:\n",
        "\n",
        "        return word2vec_model[word]\n",
        "    else:\n",
        "\n",
        "        return None\n",
        "\n",
        "#use function to get the word vector matrix for our words \n",
        "for word, i in t.word_index.items(): \n",
        "    temp = get_vector(word) \n",
        "    if temp is not None: \n",
        "        embedding_matrix[i] = temp \n",
        "\n",
        "print(word2vec_model['great']) \n",
        "print(len(word2vec_model['great']) ) #has 300 columns "
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 7.17773438e-02  2.08007812e-01 -2.84423828e-02  1.78710938e-01\n",
            "  1.32812500e-01 -9.96093750e-02  9.61914062e-02 -1.16699219e-01\n",
            " -8.54492188e-03  1.48437500e-01 -3.34472656e-02 -1.85546875e-01\n",
            "  4.10156250e-02 -8.98437500e-02  2.17285156e-02  6.93359375e-02\n",
            "  1.80664062e-01  2.22656250e-01 -1.00585938e-01 -6.93359375e-02\n",
            "  1.04427338e-04  1.60156250e-01  4.07714844e-02  7.37304688e-02\n",
            "  1.53320312e-01  6.78710938e-02 -1.03027344e-01  4.17480469e-02\n",
            "  4.27246094e-02 -1.10351562e-01 -6.68945312e-02  4.19921875e-02\n",
            "  2.50000000e-01  2.12890625e-01  1.59179688e-01  1.44653320e-02\n",
            " -4.88281250e-02  1.39770508e-02  3.55529785e-03  2.09960938e-01\n",
            "  1.52343750e-01 -7.32421875e-02  2.16796875e-01 -5.76171875e-02\n",
            " -2.84423828e-02 -3.60107422e-03  1.52343750e-01 -2.63671875e-02\n",
            "  2.13623047e-02 -1.51367188e-01  1.04003906e-01  3.18359375e-01\n",
            " -1.85546875e-01  3.68652344e-02 -1.10839844e-01 -3.17382812e-02\n",
            " -1.01562500e-01 -1.21093750e-01  3.22265625e-01 -7.32421875e-02\n",
            " -1.52343750e-01  2.67578125e-01 -1.50390625e-01 -1.23046875e-01\n",
            "  1.07910156e-01  6.68945312e-02 -2.13623047e-02 -1.00585938e-01\n",
            " -2.05078125e-01  1.17675781e-01  6.15234375e-02  6.78710938e-02\n",
            "  1.06933594e-01 -7.71484375e-02 -1.52343750e-01 -4.24194336e-03\n",
            " -1.45507812e-01  2.53906250e-01  4.80957031e-02  9.71679688e-02\n",
            " -8.36181641e-03  1.12792969e-01  5.34667969e-02  1.79443359e-02\n",
            " -5.63964844e-02 -3.30078125e-01 -9.76562500e-02  1.42578125e-01\n",
            " -1.37695312e-01  2.20947266e-02  1.00097656e-01 -5.71289062e-02\n",
            " -1.56250000e-01 -6.37817383e-03 -9.37500000e-02 -4.68750000e-02\n",
            "  8.59375000e-02  3.06640625e-01 -1.11328125e-01 -1.94335938e-01\n",
            " -2.08007812e-01  8.10546875e-02 -4.19921875e-02 -8.30078125e-02\n",
            " -1.04003906e-01  2.92968750e-01  2.39257812e-02 -3.85742188e-02\n",
            "  3.56445312e-02 -1.04980469e-01 -6.54296875e-02  2.79296875e-01\n",
            " -1.16210938e-01 -1.45874023e-02  3.84765625e-01 -7.81250000e-02\n",
            " -2.92968750e-02 -1.35742188e-01 -5.39550781e-02 -5.49316406e-02\n",
            " -8.10546875e-02 -2.88085938e-02  8.34960938e-02  2.73437500e-01\n",
            " -6.20117188e-02 -4.78515625e-02 -1.09252930e-02 -1.13769531e-01\n",
            " -1.09863281e-01  2.02148438e-01 -1.28906250e-01 -6.68945312e-02\n",
            " -2.67578125e-01  9.61914062e-02  1.04003906e-01 -1.69921875e-01\n",
            "  5.56640625e-02  1.54296875e-01  8.05664062e-02  2.19726562e-01\n",
            " -2.27539062e-01  1.10351562e-01 -8.11767578e-03 -5.63964844e-02\n",
            " -9.03320312e-02 -7.76367188e-02 -3.61328125e-02  3.61328125e-02\n",
            "  1.58203125e-01 -1.56250000e-01  2.26562500e-01  2.85156250e-01\n",
            " -5.51757812e-02  3.53515625e-01 -1.20605469e-01  1.05957031e-01\n",
            "  3.11279297e-02 -1.91406250e-01 -2.31445312e-01 -1.11816406e-01\n",
            "  2.38037109e-03  7.51953125e-02 -1.28784180e-02  1.00585938e-01\n",
            "  4.45312500e-01 -2.77343750e-01  6.68945312e-02 -8.10546875e-02\n",
            "  6.39648438e-02  1.85546875e-02 -1.11328125e-01  9.76562500e-02\n",
            "  2.06054688e-01 -1.30859375e-01  2.39257812e-02  1.10839844e-01\n",
            "  8.05664062e-02 -1.52343750e-01  4.85229492e-03  1.84326172e-02\n",
            " -9.17968750e-02 -2.41210938e-01  8.39843750e-02 -1.00585938e-01\n",
            " -1.54296875e-01  2.75878906e-02 -1.64062500e-01 -1.01562500e-01\n",
            " -6.07299805e-03  1.33514404e-03 -2.53906250e-01  3.14453125e-01\n",
            "  1.31835938e-01 -1.31835938e-01  2.17285156e-02 -1.56250000e-01\n",
            " -1.46484375e-01 -5.12695312e-02 -1.20605469e-01 -2.15820312e-01\n",
            "  3.10058594e-02  1.30859375e-01  9.71679688e-02  5.67626953e-03\n",
            "  2.20947266e-02  1.26953125e-01 -1.24511719e-02  6.15234375e-02\n",
            " -2.23388672e-02  2.50000000e-01 -7.17773438e-02  1.58203125e-01\n",
            " -7.27539062e-02  1.97753906e-02  8.85009766e-03 -9.08203125e-02\n",
            "  3.63281250e-01 -9.03320312e-02  2.41699219e-02 -1.39770508e-02\n",
            " -5.10253906e-02  2.40478516e-02  5.88989258e-03 -1.02050781e-01\n",
            " -8.85009766e-03  3.05175781e-02 -7.81250000e-02 -1.27929688e-01\n",
            "  3.85742188e-02  2.86865234e-02 -2.28515625e-01 -1.25122070e-02\n",
            "  1.54296875e-01  9.13085938e-02  1.05468750e-01 -6.44531250e-02\n",
            " -1.28906250e-01 -1.02050781e-01 -2.16064453e-02 -3.29589844e-02\n",
            "  7.47070312e-02  3.78417969e-02  7.42187500e-02 -1.23901367e-02\n",
            " -4.68750000e-02  4.88281250e-03  1.03515625e-01 -8.69140625e-02\n",
            " -2.26562500e-01 -2.53906250e-01  3.58886719e-02  4.45312500e-01\n",
            "  5.56640625e-02  1.59179688e-01  2.71484375e-01 -1.08398438e-01\n",
            "  6.25000000e-02 -5.59082031e-02 -2.50000000e-01 -1.55273438e-01\n",
            " -6.83593750e-02 -1.39648438e-01 -1.59179688e-01 -1.79443359e-02\n",
            "  2.12402344e-02  7.37304688e-02  1.30859375e-01 -8.05664062e-02\n",
            "  2.99072266e-02  1.55639648e-02 -1.66015625e-01  1.50390625e-01\n",
            " -6.77490234e-03  1.01318359e-02  1.14746094e-01 -1.48437500e-01\n",
            " -4.58984375e-02 -1.39648438e-01 -1.73828125e-01 -4.27246094e-02\n",
            " -5.81054688e-02  5.22460938e-02 -1.11328125e-01  8.44726562e-02\n",
            " -2.55126953e-02  1.40625000e-01 -1.81640625e-01  1.72119141e-02\n",
            " -1.37695312e-01 -1.47705078e-02 -1.14746094e-02  6.44531250e-02\n",
            " -2.89062500e-01 -4.80957031e-02 -1.99218750e-01 -7.12890625e-02\n",
            "  6.44531250e-02 -1.67968750e-01 -2.08740234e-02 -1.42578125e-01]\n",
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm-Dh1b9Zb_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7cb82be3-ec97-460a-f5bc-737022342207"
      },
      "source": [
        "#train model\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 0s - loss: 0.6410 - acc: 0.8571\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 0.6235 - acc: 1.0000\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 0.6065 - acc: 1.0000\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 0.5901 - acc: 1.0000\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 0.5742 - acc: 1.0000\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 0.5588 - acc: 1.0000\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 0.5439 - acc: 1.0000\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 0.5296 - acc: 1.0000\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 0.5158 - acc: 1.0000\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 0.5024 - acc: 1.0000\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 0.4895 - acc: 1.0000\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 0.4771 - acc: 1.0000\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 0.4651 - acc: 1.0000\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 0.4535 - acc: 1.0000\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 0.4423 - acc: 1.0000\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 0.4315 - acc: 1.0000\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 0.4210 - acc: 1.0000\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 0.4109 - acc: 1.0000\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 0.4012 - acc: 1.0000\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 0.3918 - acc: 1.0000\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 0.3826 - acc: 1.0000\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 0.3738 - acc: 1.0000\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 0.3653 - acc: 1.0000\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 0.3570 - acc: 1.0000\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 0.3490 - acc: 1.0000\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 0.3412 - acc: 1.0000\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 0.3337 - acc: 1.0000\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 0.3264 - acc: 1.0000\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 0.3194 - acc: 1.0000\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 0.3125 - acc: 1.0000\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 0.3059 - acc: 1.0000\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 0.2995 - acc: 1.0000\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 0.2932 - acc: 1.0000\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 0.2871 - acc: 1.0000\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 0.2813 - acc: 1.0000\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 0.2755 - acc: 1.0000\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 0.2700 - acc: 1.0000\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 0.2646 - acc: 1.0000\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 0.2594 - acc: 1.0000\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 0.2543 - acc: 1.0000\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 0.2494 - acc: 1.0000\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.2446 - acc: 1.0000\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.2399 - acc: 1.0000\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.2354 - acc: 1.0000\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.2310 - acc: 1.0000\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.2267 - acc: 1.0000\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.2225 - acc: 1.0000\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.2185 - acc: 1.0000\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.2146 - acc: 1.0000\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.2107 - acc: 1.0000\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.2070 - acc: 1.0000\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.2034 - acc: 1.0000\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.1998 - acc: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.1964 - acc: 1.0000\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.1930 - acc: 1.0000\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.1898 - acc: 1.0000\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.1866 - acc: 1.0000\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.1835 - acc: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.1805 - acc: 1.0000\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.1775 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.1746 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.1718 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.1691 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.1664 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.1638 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.1613 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.1588 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.1564 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.1540 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.1517 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.1495 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.1473 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.1451 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.1430 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.1409 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.1389 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.1370 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.1351 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.1332 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.1313 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.1295 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.1278 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.1261 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.1244 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.1227 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.1211 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.1195 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.1180 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.1164 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.1150 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.1135 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.1121 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.1107 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.1093 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.1079 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.1066 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.1053 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.1041 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.1028 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.1016 - acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f966392a390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLrc-jH6VElA",
        "colab_type": "text"
      },
      "source": [
        "**Using GloVe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5ZHPmAKWKVY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "85ba3e40-fbe3-4e93-ed30-9419670197ad"
      },
      "source": [
        "#using glove.6B.100d for pretrained embedding \n",
        "\n",
        "import numpy as np\n",
        "embedding_dict = dict()\n",
        "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word = word_vector[0]\n",
        "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32') # 100개의 값을 가지는 array로 변환\n",
        "    embedding_dict[word] = word_vector_arr\n",
        "f.close()\n",
        "\n",
        "len(embedding_dict) #has 400000 words \n",
        "\n",
        "print(embedding_dict['respectable']) #look at the embedding for respectable , has 100 length (column) , row is word "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.049773   0.19903    0.10585    0.1391    -0.32395    0.44053\n",
            "  0.3947    -0.22805   -0.25793    0.49768    0.15384   -0.08831\n",
            "  0.0782    -0.8299    -0.037788   0.16772   -0.45197   -0.17085\n",
            "  0.74756    0.98256    0.81872    0.28507    0.16178   -0.48626\n",
            " -0.006265  -0.92469   -0.30625   -0.067318  -0.046762  -0.76291\n",
            " -0.0025264 -0.018795   0.12882   -0.52457    0.3586     0.43119\n",
            " -0.89477   -0.057421  -0.53724    0.25587    0.55195    0.44698\n",
            " -0.24252    0.29946    0.25776   -0.8717     0.68426   -0.05688\n",
            " -0.1848    -0.59352   -0.11227   -0.57692   -0.013593   0.18488\n",
            " -0.32507   -0.90171    0.17672    0.075601   0.54896   -0.21488\n",
            " -0.54018   -0.45882   -0.79536    0.26331    0.18879   -0.16363\n",
            "  0.3975     0.1099     0.1164    -0.083499   0.50159    0.35802\n",
            "  0.25677    0.088546   0.42108    0.28674   -0.71285   -0.82915\n",
            "  0.15297   -0.82712    0.022112   1.067     -0.31776    0.1211\n",
            " -0.069755  -0.61327    0.27308   -0.42638   -0.085084  -0.17694\n",
            " -0.0090944  0.1109     0.62543   -0.23682   -0.44928   -0.3667\n",
            " -0.21616   -0.19187   -0.032502   0.38025  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xawu40iTRkik",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "26cb5e92-9129-48c3-c4a7-b3c1ccf26beb"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        " #make a matrix filled with zeros -> fill with the pretrained embedding matrix \n",
        " #100 since the pretrained datset has 100 dimension \n",
        "\n",
        "np.shape(embedding_matrix)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CqFT3w0XKm_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "fdde746f-5a76-4fe9-9498-c652b7621e21"
      },
      "source": [
        "print(t.word_index.items()) #tokenizer used above \n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_items([('nice', 1), ('great', 2), ('best', 3), ('amazing', 4), ('stop', 5), ('lies', 6), ('pitiful', 7), ('nerd', 8), ('excellent', 9), ('work', 10), ('supreme', 11), ('quality', 12), ('bad', 13), ('highly', 14), ('respectable', 15)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV8-WoZtXkWM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "703316bb-e488-4cbe-b824-57e7be6d9725"
      },
      "source": [
        "\n",
        "for word, i in t.word_index.items():  #get key, value from dictionary \n",
        "    temp = embedding_dict.get(word)  #get the embedding values for each word from pretrained matrix \n",
        "\n",
        "    if temp is not None: \n",
        "        embedding_matrix[i] = temp #if the word is inside the pretrained matrix -- use the value and implement to embedding matrix \n",
        "\n",
        "\n",
        "\n",
        "embedding_matrix[:1]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvgSkS5BYQsV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0328da99-1235-4699-c118-3d5d06146592"
      },
      "source": [
        "\n",
        "#create model \n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False) #the weights are now the pretrained model \n",
        "\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)\n",
        "\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 0s - loss: 0.8681 - acc: 0.5714\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 0.8408 - acc: 0.4286\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 0.8144 - acc: 0.4286\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 0.7888 - acc: 0.4286\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 0.7641 - acc: 0.4286\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 0.7403 - acc: 0.4286\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 0.7175 - acc: 0.4286\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 0.6957 - acc: 0.4286\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 0.6749 - acc: 0.4286\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 0.6551 - acc: 0.4286\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 0.6362 - acc: 0.5714\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 0.6184 - acc: 0.5714\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 0.6014 - acc: 0.7143\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 0.5854 - acc: 0.7143\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 0.5702 - acc: 0.7143\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 0.5558 - acc: 0.7143\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 0.5422 - acc: 0.7143\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 0.5293 - acc: 0.7143\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 0.5171 - acc: 0.7143\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 0.5054 - acc: 0.7143\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 0.4943 - acc: 0.7143\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 0.4837 - acc: 0.7143\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 0.4736 - acc: 0.7143\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 0.4640 - acc: 0.7143\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 0.4547 - acc: 0.7143\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 0.4458 - acc: 0.7143\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 0.4372 - acc: 0.7143\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 0.4290 - acc: 0.7143\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 0.4211 - acc: 0.7143\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 0.4134 - acc: 0.7143\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 0.4060 - acc: 0.7143\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 0.3989 - acc: 0.7143\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 0.3920 - acc: 0.7143\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 0.3853 - acc: 0.7143\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 0.3788 - acc: 0.8571\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 0.3725 - acc: 0.8571\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 0.3664 - acc: 0.8571\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 0.3605 - acc: 0.8571\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 0.3548 - acc: 0.8571\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 0.3492 - acc: 0.8571\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 0.3438 - acc: 0.8571\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.3386 - acc: 0.8571\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.3335 - acc: 0.8571\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.3286 - acc: 0.8571\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.3238 - acc: 0.8571\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.3191 - acc: 1.0000\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.3146 - acc: 1.0000\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.3102 - acc: 1.0000\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.3060 - acc: 1.0000\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.3018 - acc: 1.0000\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.2978 - acc: 1.0000\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.2939 - acc: 1.0000\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.2901 - acc: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.2864 - acc: 1.0000\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.2828 - acc: 1.0000\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.2793 - acc: 1.0000\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.2759 - acc: 1.0000\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.2726 - acc: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.2694 - acc: 1.0000\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.2662 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.2632 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.2602 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.2573 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.2545 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.2517 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.2491 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.2465 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.2439 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.2414 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.2390 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.2366 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.2343 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.2321 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.2299 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.2277 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.2256 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.2236 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.2216 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.2196 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.2177 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.2158 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.2140 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.2122 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.2104 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.2087 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.2070 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.2054 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.2038 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.2022 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.2007 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.1992 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.1977 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.1963 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.1948 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.1934 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.1921 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.1907 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.1894 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.1881 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.1869 - acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f97831b5128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    }
  ]
}